{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome to AZ-400 learning","text":"<p>Notes for taking AZ-400 exam.</p>"},{"location":"#microsoft-learning-path","title":"Microsoft learning path","text":"<ul> <li>DevOps transformation journey</li> <li>Development for enterprise DevOps</li> <li>Implement CI with Azure Pipelines and GitHub Actions</li> <li>Design and implement a release strategy</li> <li>Implement a secure continuous deployment using Azure Pipelines</li> <li>Manage infrastructure as code using Azure and DSC</li> <li>Design and implement a dependency management strategy</li> <li>Implement continuous feedback</li> <li>Implement security and validate code bases for compliance</li> </ul>"},{"location":"Design-and-implement-a-dependency-management-strategy/GitHub%20packages/","title":"GitHub packages","text":""},{"location":"Design-and-implement-a-dependency-management-strategy/GitHub%20packages/#publishing-packages","title":"Publishing packages","text":"<p>GitHub Packages use native package tooling commands to publish and install package versions.</p> <p>Support for package registries:</p> Language Package format Package client JavaScript package.json npm Ruby Gemfile gem Java pom.xml mvn Java build.gradle or build.gradle.kts gradle .NET nupkg dotnet CLI N/A Dockerfile Docker <p>Using any supported package client, to publish your package, you need to:</p> <ol> <li>Create or use an existing access token with the appropriate scopes for the task you want to accomplish: Creating a personal access token. When you create a personal access token (PAT), you can assign the token to different scopes depending on your needs. </li> <li>Authenticate to GitHub Packages using your access token and the instructions for your package client.</li> <li>Publish the package using the instructions for your package client.</li> </ol>"},{"location":"Design-and-implement-a-dependency-management-strategy/GitHub%20packages/#installing-packages","title":"Installing packages","text":"<p>You can install any package you have permission to view from GitHub Packages and use the package as a dependency in your project. After you find a package, read the package's installation and description instructions on the package page. You can install a package using any supported package client following the same general guidelines.</p> <ol> <li>Authenticate to GitHub Packages using the instructions for your package client.</li> <li>Install the package using the instructions for your package client.</li> </ol>"},{"location":"Design-and-implement-a-dependency-management-strategy/GitHub%20packages/#deleting-and-restoring-a-package","title":"Deleting and restoring a package","text":"<p>You can delete it on GitHub if you have the required access:</p> <ul> <li>An entire private package.</li> <li>If there aren't more than 5000 downloads of any version of the package, an entire public package.</li> <li>A specific version of a private package.</li> <li>A specific version of a public package if the package version doesn't have more than 5000 downloads.</li> <li>For packages that inherit their access permissions from repositories, you can delete a package if you have admin permissions to the repository.</li> </ul> <p>You can also restore an entire package or package version, if:</p> <ul> <li>You restore the package within 30 days of its deletion.</li> <li>The same package namespace is still available and not used for a new package.</li> </ul> <p>You can use the REST API to manage your packages.</p>"},{"location":"Design-and-implement-a-dependency-management-strategy/GitHub%20packages/#package-access-control-and-visibility","title":"Package access control and visibility","text":"Permission Access description read Can download the package. Can read package metadata. write Can upload and download this package. Can read and write package metadata. admin Can upload, download, delete, and manage this package. Can read and write package metadata. Can grant package permissions."},{"location":"Design-and-implement-a-dependency-management-strategy/Implementing%20a%20versioning%20strategy/","title":"Implementing a versioning strategy","text":""},{"location":"Design-and-implement-a-dependency-management-strategy/Implementing%20a%20versioning%20strategy/#release-views","title":"Release views","text":"<p>Feeds in Azure Artifacts have three different views by default. These views are added when a new feed is created. The three views are:</p> <ul> <li>Local: The <code>@Local</code> view contains all release and prerelease packages and the packages downloaded from upstream sources.</li> <li>Prerelease: The <code>@Prerelease</code> view contains all packages that have a label in their version number.</li> <li>Release: The <code>@Release</code> view contains all packages that are considered official releases.</li> </ul>"},{"location":"Design-and-implement-a-dependency-management-strategy/Implementing%20a%20versioning%20strategy/#using-views","title":"Using views","text":"<p>You can use views to offer help consumers of a package feed filter between released and unreleased versions of packages. Essentially, it allows a consumer to make a conscious decision to choose from released packages or opt-in to prereleases of a certain quality level.</p> <p>By default, the <code>@Local</code> view is used to offer the list of available packages. The format for this URI is:</p> <pre><code>https://pkgs.dev.azure.com/{yourteamproject}/_packaging/{feedname}/nuget/v3/index.json\n</code></pre>"},{"location":"Design-and-implement-a-dependency-management-strategy/Implementing%20a%20versioning%20strategy/#promoting-packages","title":"Promoting packages","text":"<p>Release and Prerelease's two views might be sufficient, but you can create more views when you want finer-grained quality levels if necessary, such as <code>alpha</code> and <code>beta</code>.</p> <p>Packages will always show in the Local view, but only in a particular view after being promoted. Depending on the URL used to connect to the feed, the available packages will be listed.</p> <p>Upstream sources will only be evaluated when using the @Local view of the feed.</p>"},{"location":"Design-and-implement-a-dependency-management-strategy/Migrate%20consolidating%20and%20secure%20artifacts/","title":"Migrate consolidating and secure artifacts","text":"<p>An artifact is a deployable component of your application. Azure Pipelines can work with a wide variety of artifact sources and repositories.</p> <p>When you're creating a release pipeline, you need to link the required artifact sources. Often, it will represent the output of a build pipeline from a continuous integration system like Azure Pipelines, Jenkins, or TeamCity.</p> <p>Azure Artifacts are one of the services that's part of Azure DevOps. Using it can eliminate the need to manage file shares or host private package services. It lets you share code easily by allowing you to store Maven, npm, or NuGet packages together, cloud-hosted, indexed and matched.</p>"},{"location":"Design-and-implement-a-dependency-management-strategy/Migrate%20consolidating%20and%20secure%20artifacts/#secure-access-to-package-feeds","title":"Secure access to package feeds","text":""},{"location":"Design-and-implement-a-dependency-management-strategy/Migrate%20consolidating%20and%20secure%20artifacts/#trusted-sources","title":"Trusted sources","text":"<p>Package feeds are a trusted source of packages. The offered packages will be consumed by other codebases and used to build software that needs to be secure.</p>"},{"location":"Design-and-implement-a-dependency-management-strategy/Migrate%20consolidating%20and%20secure%20artifacts/#securing-access","title":"Securing access","text":"<p>Package feeds must be secured for access by authorized accounts, so only verified and trusted packages are stored there. None should push packages to a feed without the proper role and permissions.</p>"},{"location":"Design-and-implement-a-dependency-management-strategy/Migrate%20consolidating%20and%20secure%20artifacts/#securing-availability","title":"Securing availability","text":"<p>Another aspect of security for package feeds is about the public or private availability of the packages. The feeds of public sources are available for anonymous consumption. Private feeds have restricted access most of the time.</p>"},{"location":"Design-and-implement-a-dependency-management-strategy/Migrate%20consolidating%20and%20secure%20artifacts/#azure-artifacts-roles","title":"Azure Artifacts roles","text":"<ul> <li>Reader: Can list and restore (or install) packages from the feed.</li> <li>Collaborator: Can save packages from upstream sources.</li> <li>Contributor: Can push and unlist packages in the feed.</li> <li>Owner: Has all available permissions for a package feed.</li> </ul>"},{"location":"Design-and-implement-a-dependency-management-strategy/Package%20dependencies/","title":"Package dependencies","text":""},{"location":"Design-and-implement-a-dependency-management-strategy/Package%20dependencies/#elements-of-a-dependency-management-strategy","title":"Elements of a dependency management strategy","text":"<ul> <li>Standardization Managing dependencies benefit from a standardized way of declaring and resolving them in your codebase. Standardization allows a repeatable, predictable process and usage that can be automated as well.</li> <li>Package formats and sources The distribution of dependencies can be performed by a packaging method suited for your solution's dependency type. Each dependency is packaged using its usable format and stored in a centralized source. Your dependency management strategy should include the selection of package formats and corresponding sources where to store and retrieve packages.</li> <li>Versioning Just like your own code and components, the dependencies in your solution usually evolve. While your codebase grows and changes, you need to consider the changes in your dependencies as well. It requires a versioning mechanism for the dependencies to be selective of the version of a dependency you want to use.</li> </ul>"},{"location":"Design-and-implement-a-dependency-management-strategy/Package%20dependencies/#source-and-package-componentization","title":"Source and package componentization","text":"<ul> <li>Source componentization The first way of componentization is focused on source code. It refers to splitting the source code in the codebase into separate parts and organizing it around the identified components. It works if the source code isn't shared outside of the project. Once the components need to be shared, it requires distributing the source code or the produced binary artifacts created from it.</li> <li>Package componentization The second way uses packages. Distributing software components is performed utilizing packages as a formal way of wrapping and handling the components. A shift to packages adds characteristics needed for proper dependency management, like tracking and versioning packages in your solution.</li> </ul>"},{"location":"Design-and-implement-a-dependency-management-strategy/Package%20dependencies/#scanning-codebase-for-dependencies","title":"Scanning codebase for dependencies","text":"<ul> <li>Duplicate code: When certain pieces of code appear in several places, it's a good indication that this code can be reused. Keep in mind that code duplication isn't necessarily a bad practice. However, if the code can be made available properly, it does have benefits over copying code and must manage that. The first step to isolate these pieces of duplicate code is to centralize them in the codebase and componentize them in the appropriate way for the type of code.</li> <li>High cohesion and low coupling: A second approach is to find code that might define components in your solution. You'll look for code elements that have high cohesion and low coupling with other parts of code. It could be a specific object model with business logic or code related to its responsibility, such as a set of helper or utility codes or perhaps a basis for other code to be built upon.</li> <li>Individual lifecycle: Related to high cohesion, you can look for parts of the code that have a similar lifecycle and can be deployed and released individually. If such code can be maintained by a team separate from the codebase that it's currently in, it's an indication that it could be a component outside of the solution.</li> <li>Stable parts: Some parts of your codebase might have a slow rate of change. That code is stable and isn't altered often. You can check your code repository to find the code with a low change frequency.</li> <li>Independent code and components: Whenever code and components are independent and unrelated to other parts of the system, they can be isolated to a separate component and dependency.</li> </ul>"},{"location":"Design-and-implement-a-dependency-management-strategy/Understanding%20package%20management/","title":"Understanding package management","text":""},{"location":"Design-and-implement-a-dependency-management-strategy/Understanding%20package%20management/#packages","title":"Packages","text":"<p>A package is a formalized way of creating a distributable unit of software artifacts that can be consumed from another software solution.</p> <p>The package describes the content it contains and usually provides extra metadata, and the information uniquely identifies the individual packages and is self-descriptive.</p> <p>Packages are used to define the components you rely on and depend upon in your software solution.</p>"},{"location":"Design-and-implement-a-dependency-management-strategy/Understanding%20package%20management/#types-of-packages","title":"Types of packages","text":"<p>Over the past years, the packaging formats have changed and evolved. Now there are a couple of de facto standard formats for packages.</p> <ul> <li>NuGet packages are a standard used for .NET code artifacts. It includes .NET assemblies and related files, tooling, and sometimes only metadata. NuGet defines the way packages are created, stored, and consumed. A NuGet package is essentially a compressed folder structure with files in ZIP format and has the <code>.nupkg</code> extension.</li> <li>npm package is used for JavaScript development. It originates from node.js development, where it's the default packaging format. An npm package is a file or folder containing JavaScript files and a <code>package.json</code> file describing the package's metadata. For node.js, the package usually includes one or more modules that can be loaded once the package is consumed. </li> <li>Maven is used for Java-based projects. Each package has a Project Object Model file describing the project's metadata and is the basic unit for defining a package and working with it.</li> <li>PyPi The Python Package Index, abbreviated as PyPI and known as the Cheese Shop, is the official third-party software repository for Python.</li> <li>Docker packages are called images and contain complete and self-contained deployments of components. A Docker image commonly represents a software component that can be hosted and executed by itself without any dependencies on other images. Docker images are layered and might be dependent on other images as their basis. Such images are referred to as base images.</li> </ul>"},{"location":"Design-and-implement-a-dependency-management-strategy/Understanding%20package%20management/#package-feeds","title":"Package feeds","text":"<p>Packages should be stored in a centralized place for distribution and consumption to take dependencies on the components it contains. The centralized storage for packages is commonly called a package feed. There are other names in use, such as repository or registry.</p>"},{"location":"Design-and-implement-a-dependency-management-strategy/Understanding%20package%20management/#azure-artifacts","title":"Azure Artifacts","text":"<p>Azure Artifacts currently supports feeds that can store five different package types:</p> <ul> <li>NuGet packages.</li> <li>npm packages.</li> <li>Maven.</li> <li>Universal packages.</li> <li>Python.</li> </ul>"},{"location":"Design-and-implement-a-dependency-management-strategy/Understanding%20package%20management/#lab-address","title":"Lab address","text":"<p>https://microsoftlearning.github.io/AZ400-DesigningandImplementingMicrosoftDevOpsSolutions/Instructions/Labs/AZ400_M08_L15_Package_Management_with_Azure_Artifacts.html</p>"},{"location":"Design-and-implement-a-release-strategy/Automating%20inspection%20of%20health/","title":"Automating inspection of health","text":""},{"location":"Design-and-implement-a-release-strategy/Automating%20inspection%20of%20health/#release-gates","title":"Release gates","text":"<p>Release gates allow the automatic collection of health signals from external services and then promote the release when all the signs are booming simultaneously or stop the deployment on timeout. Typically, gates are connected with incident management, problem management, change management, monitoring, and external approval systems.</p>"},{"location":"Design-and-implement-a-release-strategy/Automating%20inspection%20of%20health/#events-subscriptions-and-notifications","title":"Events, subscriptions, and notifications","text":"<p>Events are raised when specific actions occur, like when a release is started or a build is completed. A notification subscription is associated with a supported event type. The subscription ensures you get notified when a specific event occurs. Notifications are usually emails that you receive when an event occurs to which you're subscribed.</p>"},{"location":"Design-and-implement-a-release-strategy/Automating%20inspection%20of%20health/#service-hooks","title":"Service hooks","text":"<p>Service hooks enable you to do tasks on other services when events happen in your Azure DevOps Services projects. For example, create a card in Trello when a work item is created or send a push notification to your team's Slack when a build fails. Service hooks can also be used in custom apps and services as a more efficient way to drive activities when events happen in your projects.</p>"},{"location":"Design-and-implement-a-release-strategy/Automating%20inspection%20of%20health/#built-in-support-for-the-service-hooks-in-azure","title":"Built-in support for the service hooks in Azure","text":"Build and release Collaborate Customer support Plan and track Integrate AppVeyor Campfire UserVoice Trello Azure Service Bus Bamboo Flowdock Zendesk - Azure Storage Jenkins HipChat - - Web Hooks MyGet Hubot - - Zapier Slack - - - -"},{"location":"Design-and-implement-a-release-strategy/Automating%20inspection%20of%20health/#reporting","title":"Reporting","text":"<p>Reporting is the most static approach to inspection but also the most evident in many cases. Creating a dashboard that shows the status of your build and releases combined with team-specific information is, in many cases, a valuable asset to get insights.</p>"},{"location":"Design-and-implement-a-release-strategy/Creating%20a%20release%20pipeline/","title":"Creating a release pipeline","text":""},{"location":"Design-and-implement-a-release-strategy/Creating%20a%20release%20pipeline/#release-pipeline-capabilities","title":"Release pipeline capabilities","text":"Feature Description Agents Specifies a required resource on which the pipeline runs. Approvals Defines a set of validations required before completing a deployment stage. Artifacts Supports publishing or consuming different package types. Caching Reduces build time by allowing outputs or downloaded dependencies from one run to be reused in later runs. In Preview, available with Azure Pipelines only. Conditions Specifies conditions to be met before running a job. Container jobs Specifies jobs to run in a container. Demands Ensures pipeline requirements are met before running a pipeline stage. Requires self-hosted agents. Dependencies Specifies a requirement that must be met to run the next job or stage. Deployment groups Defines a logical set of deployment target machines. Deployment group jobs Specifies a job to release to a deployment group. Deployment jobs Defines the deployment steps. Requires Multi-stage pipelines experience. Environment Represents a collection of resources targeted for deployment. Available with Azure Pipelines only. Gates Supports automatic collection and evaluation of external health signals before completing a release stage. Available with Azure Pipelines only. Jobs Defines the execution sequence of a set of steps. Service connections Enables a connection to a remote service that is required to execute tasks in a job. Service containers Enables you to manage the lifecycle of a containerized service. Stages Organizes jobs within a pipeline. Task groups Encapsulates a sequence of tasks into a single reusable task. If using YAML, see templates. Tasks Defines the building blocks that make up a pipeline. Templates Defines reusable content, logic, and parameters. Triggers Defines the event that causes a pipeline to run. Variables Represents a value to be replaced by data to pass to the pipeline. Variable groups Use to store values that you want to control and make available across multiple pipelines."},{"location":"Design-and-implement-a-release-strategy/Creating%20a%20release%20pipeline/#exploring-release-pipelines","title":"Exploring release pipelines","text":"<p>A release pipeline takes artifacts and releases them through stages and finally into production.</p> <p></p> <p>The first component in a release pipeline is an artifact:</p> <ul> <li>Artifacts can come from different sources.</li> <li>The most common source is a package from a build pipeline.</li> <li>Another commonly seen artifact source is, for example, source control.</li> </ul> <p>Furthermore, a release pipeline has a trigger: the mechanism that starts a new release.</p> <p>A trigger can be:</p> <ul> <li>A manual trigger, where people start to release by hand.</li> <li>A scheduled trigger, where a release is triggered based on a specific time.</li> <li>A continuous deployment trigger, where another event triggers a release. For example, a completed build.</li> </ul> <p>Another vital component of a release pipeline is stages or sometimes called environments. It's where the artifact will be eventually installed. For example, the artifact contains the compiled website installed on the webserver or somewhere in the cloud. You can have many stages (environments); part of the release strategy is finding the appropriate combination of stages.</p> <p>Another component of a release pipeline is approval. People often want to sign a release before installing it in the environment. In more mature organizations, this manual approval process can be replaced by an automatic process that checks the quality before the components move on to the next stage.</p> <p>Finally, we have the tasks within the various stages. The tasks are the steps that need to be executed to install, configure, and validate the installed artifact.</p>"},{"location":"Design-and-implement-a-release-strategy/Creating%20a%20release%20pipeline/#release-jobs","title":"Release jobs","text":"<p>You can organize your build or release pipeline into jobs. Every build or deployment pipeline has at least one job. A job is a series of tasks that run sequentially on the same target. It can be a Windows server, a Linux server, a container, or a deployment group. A release job is executed by a build/release agent. This agent can only run one job at the same time. You specify a series of tasks you want to run on the same agent during your job design. When the build or release pipeline is triggered at runtime, each job is dispatched to its target as one or more.</p>"},{"location":"Design-and-implement-a-release-strategy/Creating%20a%20release%20pipeline/#lab-address","title":"Lab address","text":"<p>https://aka.ms/az-400-configure-pipelines-as-code-with-yaml</p>"},{"location":"Design-and-implement-a-release-strategy/Introduction%20Continuous%20Delivery/","title":"Introduction Continuous Delivery","text":""},{"location":"Design-and-implement-a-release-strategy/Introduction%20Continuous%20Delivery/#continuous-delivery","title":"Continuous Delivery","text":"<p>Continuous Delivery (CD) is a set of processes, tools, and techniques for rapid, reliable, and continuous software development and delivery. It means that Continuous Delivery goes beyond the release of software through a pipeline. The pipeline is a crucial component and the focus of this course, but continuous delivery is more.</p> <p>Eight principles of Continuous Delivery:</p> <ul> <li>The process for releasing/deploying software must be repeatable and reliable.</li> <li>Automate everything.</li> <li>If something is difficult or painful, do it more often.</li> <li>Keep everything in source control.</li> <li>Done means \"released.\"</li> <li>Build quality in.</li> <li>Everybody has responsibility for the release process.</li> <li>Improve continuously.</li> </ul> <p>To deploy more often, we need to reconsider our:</p> <ul> <li>Software architecture (monoliths are hard to deploy).</li> <li>Testing strategy (manual tests don't scale well).</li> <li>Organization (separated business and IT departments don't work smoothly), and so forth.</li> </ul>"},{"location":"Design-and-implement-a-release-strategy/Introduction%20Continuous%20Delivery/#moving-to-continuous-delivery","title":"Moving to Continuous Delivery","text":"<p>We want to have feedback loops or quality gates in place. A feedback loop can be different things:</p> <ul> <li>A unit test to validate the code.</li> <li>An automated build to validate the sources.</li> <li>An automated test on a Test environment.</li> <li>Some monitor on a server.</li> <li>Usage instrumentation in the code.</li> </ul>"},{"location":"Design-and-implement-a-release-strategy/Introduction%20Continuous%20Delivery/#understanding-release-and-deployment","title":"Understanding release and deployment","text":"<p>A release is a package or container containing a versioned set of artifacts specified in a release pipeline in your CI/CD process. It includes a snapshot of all the information required to carry out all the tasks and actions in the release pipeline, such as:</p> <ul> <li>The stages or environments.</li> <li>The tasks for each one.</li> <li>The values of task parameters and variables.</li> <li>The release policies such as triggers, approvers, and release queuing options.</li> <li>There can be multiple releases from one release pipeline (or release process).</li> </ul> <p>Deployment is the action of running the tasks for one stage, which results in a tested and deployed application and other activities specified for that stage. Starting a release starts each deployment based on the settings and policies defined in the original release pipeline. There can be multiple deployments of each release, even for one stage.</p> <p>When a release deployment fails for a stage, you can redeploy the same release to that stage.</p>"},{"location":"Design-and-implement-a-release-strategy/Introduction%20Continuous%20Delivery/#understanding-release-process-versus-release","title":"Understanding release process versus release","text":"<p>The release is an instance of the release pipeline. You can compare it with object instantiation.</p> <p></p> <p>In Object Orientation, a class contains the blueprint or definition of an object. But the object itself is an instance of that blueprint.</p>"},{"location":"Design-and-implement-a-release-strategy/Managing%20and%20modularizing%20tasks%20and%20templates/","title":"Managing and modularizing tasks and templates","text":""},{"location":"Design-and-implement-a-release-strategy/Managing%20and%20modularizing%20tasks%20and%20templates/#task-groups","title":"Task groups","text":"<p>A task group allows you to encapsulate a sequence of tasks, already defined in a build or a release pipeline, into a single reusable task that can be added to a build or release pipeline, just like any other task.</p> <p>Task groups are a way to standardize and centrally manage deployment steps for all your applications.</p>"},{"location":"Design-and-implement-a-release-strategy/Managing%20and%20modularizing%20tasks%20and%20templates/#variables-in-release-pipelines","title":"Variables in release pipelines","text":""},{"location":"Design-and-implement-a-release-strategy/Managing%20and%20modularizing%20tasks%20and%20templates/#predefined-variables","title":"Predefined variables","text":"<p>When running your release pipeline, you always need variables that come from the agent or context of the release pipeline. For example, the agent directory where the sources are downloaded, the build number or build ID, the agent's name, or any other information. This information is accessible in predefined variables that you can use in your tasks.</p>"},{"location":"Design-and-implement-a-release-strategy/Managing%20and%20modularizing%20tasks%20and%20templates/#release-pipeline-variables","title":"Release pipeline variables","text":"<p>Choose a release pipeline variable when you need to use the same value across all the stages and tasks in the release pipeline, and you want to change the value in a single place.</p>"},{"location":"Design-and-implement-a-release-strategy/Managing%20and%20modularizing%20tasks%20and%20templates/#stage-variables","title":"Stage variables","text":"<p>Share values across all the tasks within one specific stage by using stage variables. Use a stage-level variable for values that vary from stage to stage (and are the same for all the tasks in a stage).</p>"},{"location":"Design-and-implement-a-release-strategy/Managing%20and%20modularizing%20tasks%20and%20templates/#normal-and-secret-variables","title":"Normal and secret variables","text":"<p>Because the pipeline tasks are executed on an agent, variable values are passed to the various tasks using environment variables. The task knows how to read it. You should be aware that a variable contains clear text and can be exposed to the target system. When you use the variable in the log output, you can also see the variable's value. When the pipeline has finished, the values will be cleared. You can mark a variable in the release pipeline as secret. This way, the secret is hidden from the log output. It's beneficial when writing a password or other sensitive information.</p>"},{"location":"Design-and-implement-a-release-strategy/Managing%20and%20modularizing%20tasks%20and%20templates/#variable-groups","title":"Variable groups","text":"<p>A variable group stores values that you want to make available across multiple builds and release pipelines.</p> <p>Examples:</p> <ul> <li>Store the username and password for a shared server.</li> <li>Store a share connection string.</li> <li>Store the geolocation of an application.</li> <li>Store all settings for a specific application.</li> </ul>"},{"location":"Design-and-implement-a-release-strategy/Provisioning%20and%20testing%20environments/","title":"Provisioning and testing environments","text":""},{"location":"Design-and-implement-a-release-strategy/Provisioning%20and%20testing%20environments/#deployment-targets","title":"Deployment targets","text":"<p>When we focus on the deployment of the Infrastructure, we should first consider the differences between the target environments that we can deploy to:</p> <ul> <li>On-Premises servers.</li> <li>Cloud servers or Infrastructure as a Service (IaaS). For example, Virtual machines or networks.</li> <li>Platform as a Service (PaaS) and Functions as a Service (FaaS). For example, Azure SQL Database in both PaaS and serverless options.</li> <li>Clusters.</li> <li>Service Connections.</li> </ul>"},{"location":"Design-and-implement-a-release-strategy/Provisioning%20and%20testing%20environments/#configuring-automated-integration-and-functional-test-automation","title":"Configuring automated integration and functional test automation","text":"<p>We can make four quadrants where each side of the square defines our targets with our tests:</p> <ul> <li>Business facing: the tests are more functional and often executed by end users of the system or by specialized testers that know the problem domain well.</li> <li>Supporting the Team: it helps a development team get constant feedback on the product to find bugs quickly and deliver a quality build-in product.</li> <li>Technology facing: the tests are rather technical and non-meaningful to business people. They're typical tests written and executed by the developers in a development team.</li> <li>Critique Product: tests that validate a product's workings on its functional and non-functional requirements.</li> </ul>"},{"location":"Design-and-implement-a-release-strategy/Provisioning%20and%20testing%20environments/#shift-left","title":"Shift-left","text":"<p>The goal for shifting left is to move quality upstream by performing tests early in the pipeline. It represents the phrase \"fail fast, fail often\" combining test and process improvements reduces the time it takes for tests to be run and the impact of failures later on.</p>"},{"location":"Design-and-implement-a-release-strategy/Provisioning%20and%20testing%20environments/#azure-load-testing","title":"Azure Load Testing","text":"<p>Azure Load Testing Preview is a fully managed load-testing service that enables you to generate a high-scale load.</p> <ul> <li>The service simulates your applications' traffic, helping you optimize application performance, scalability, or capacity.</li> <li>You can create a load test using existing test scripts based on Apache JMeter. Azure Load Testing abstracts the infrastructure to run your JMeter script and load test your application.</li> <li>Azure Load Testing collects detailed resource metrics for Azure-based applications to help you identify performance bottlenecks across your Azure application components.</li> <li>You can automate regression testing by running load tests as part of your continuous integration and continuous deployment (CI/CD) workflow.</li> </ul>"},{"location":"Design-and-implement-a-release-strategy/Provisioning%20and%20testing%20environments/#lab-addresses","title":"Lab addresses","text":"<p>https://aka.ms/az-400-set-up-and-run-functional-tests</p>"},{"location":"Design-and-implement-a-release-strategy/Release%20recommendations/","title":"Release recommendations","text":""},{"location":"Design-and-implement-a-release-strategy/Release%20recommendations/#release-gates","title":"Release gates","text":"<p>Release gates give you more control over the start and completion of the deployment pipeline. When the release starts, it checks the state of the gate by calling an API. If the \"gate\" is open, we can continue. Otherwise, we'll stop the release. By using scripts and APIs, you can create your release gates instead of manual approval. Or at least extending your manual approval.</p> <p>Other scenarios for automatic approvals are, for example:</p> <ul> <li>Incident and issues management. Ensure the required status for work items, incidents, and issues. For example, ensure that deployment only occurs if no bugs exist.</li> <li>Notify users such as legal approval departments, auditors, or IT managers about a deployment by integrating with approval collaboration systems such as Microsoft Teams or Slack and waiting for the approval to complete.</li> <li>Quality validation. Query metrics from tests on the build artifacts such as pass rate or code coverage and only deploy within required thresholds.</li> <li>Security scan on artifacts. Ensure security scans such as anti-virus checking, code signing, and policy checking for build artifacts have been completed. A gate might start the scan and wait for it to finish or check for completion.</li> <li>User experience relative to baseline. Using product telemetry, ensure the user experience hasn't regressed from the baseline state. The experience level before the deployment could be considered a baseline.</li> <li>Change management. Wait for change management procedures in a system such as ServiceNow complete before the deployment occurs.</li> <li>Infrastructure health. Execute monitoring and validate the infrastructure against compliance rules after deployment or wait for proper resource use and a positive security report.</li> </ul> <p>A quality gate is located before a stage that is dependent on the outcome of a previous stage. A quality gate was typically something that a QA department monitored in the past.</p>"},{"location":"Design-and-implement-a-release-strategy/Release%20recommendations/#lab-address","title":"Lab address","text":"<p>https://aka.ms/az-400-control-deployments-using-release-gates</p>"},{"location":"DevOps-transformation-journey/Agile%20planning%20with%20GitHub%20Projects%20and%20Azure%20Boards/","title":"Agile planning with GitHub Projects and Azure Boards","text":""},{"location":"DevOps-transformation-journey/Agile%20planning%20with%20GitHub%20Projects%20and%20Azure%20Boards/#project-boards","title":"Project Boards","text":"<p>There are three different types of project boards:</p> <ul> <li>User-owned project boards: contain issues and pull requests from any personal repository.</li> <li>Organization-wide project boards: contain issues and pull requests from any repository that belongs to an organization.</li> <li>Repository project boards: scoped to issues and pull requests within single repository.</li> </ul> <p>You can use templates to quickly set up a new board:</p> Template name Description Basic kanban To do, In progress, Done columns. Automated kanban Automatic moves between To do, In progress, Done columns. Automated kanban with review Cards automatically move between: To do, In progress, and Done columns, with extra triggers for pull request review status. Bug triage Triage and prioritize bugs with: To do, High priority, Low priority, and Closed columns."},{"location":"DevOps-transformation-journey/Agile%20planning%20with%20GitHub%20Projects%20and%20Azure%20Boards/#projects","title":"Projects","text":"<p>Customizable and flexible tool version of projects for planning and tracking on GitHub.</p>"},{"location":"DevOps-transformation-journey/Agile%20planning%20with%20GitHub%20Projects%20and%20Azure%20Boards/#azure-boards","title":"Azure Boards","text":"<p>Customizable tool to manage software projects supporting Agile, Scrum and Kanban processes by default. May contain Delivery plans, used to view deliveries.</p>"},{"location":"DevOps-transformation-journey/Choosing%20the%20DevOps%20tools/","title":"Choosing the DevOps tools","text":""},{"location":"DevOps-transformation-journey/Choosing%20the%20DevOps%20tools/#what-does-azure-devops-provide","title":"What does Azure DevOps provide?","text":"<ul> <li>Azure Boards: agile planning, work item tracking, visualization, reporting tool.</li> <li>Azure Pipelines: cloud-agnostic CI/CD platform supporting containers or Kubernetes.</li> <li>Azire Repos: cloud-hosted private git repos.</li> <li>Azure Artifacts: integrated package management with support for Maven, npm, Python and NuGet package feeds from public or private sources.</li> <li>Azure Test Plans: integrated planned and exploratory testing solution.</li> </ul>"},{"location":"DevOps-transformation-journey/Choosing%20the%20DevOps%20tools/#some-github-services","title":"Some GitHub services","text":"<ul> <li>Codespaces: cloud-hosted IDE operated from web browser.</li> <li>Actions: allows creation of automation workflows.</li> <li>Packages: easy intgration with open-source and third-party offerings.</li> <li>Security: detailed code scanning and review features.</li> </ul>"},{"location":"DevOps-transformation-journey/Choosing%20the%20DevOps%20tools/#auth-and-access-strategies","title":"Auth and access strategies","text":"<ul> <li>Personal access tokens: use when tools don't natively support Microsoft accounts or Azure AD for authentication. Use in XCode, NuGet, Git-based repos.</li> <li>Security groups.</li> <li>Multifactor Auth.</li> </ul>"},{"location":"DevOps-transformation-journey/Choosing%20the%20right%20project/","title":"Choosing the right project","text":""},{"location":"DevOps-transformation-journey/Choosing%20the%20right%20project/#greenfield-brownfield-projects","title":"Greenfield / Brownfield projects","text":""},{"location":"DevOps-transformation-journey/Choosing%20the%20right%20project/#greenfield","title":"Greenfield","text":"<ul> <li>Clean slate,</li> <li>More accessible at the beginning,</li> <li>Avoiding existing bbusiness processess that don't align with project needs.</li> </ul>"},{"location":"DevOps-transformation-journey/Choosing%20the%20right%20project/#brownfield","title":"Brownfield","text":"<ul> <li>Existing codebase,</li> <li>Siginificant ammount of technical debt,</li> <li>Existing teams.</li> </ul>"},{"location":"DevOps-transformation-journey/Choosing%20the%20right%20project/#systems-of-record-systems-of-engagement","title":"Systems of record / Systems of engagement","text":""},{"location":"DevOps-transformation-journey/Choosing%20the%20right%20project/#systems-of-record","title":"Systems of record","text":"<ul> <li>Provide the truth about data elements,</li> <li>Evolving slowly and carefully.</li> </ul> <p>For instance, a banking system.</p>"},{"location":"DevOps-transformation-journey/Choosing%20the%20right%20project/#systems-of-engagement","title":"Systems of engagement","text":"<ul> <li>Modified regularly,</li> <li>More exploratory,</li> <li>Use experimentation to solve new problems.</li> </ul>"},{"location":"DevOps-transformation-journey/Choosing%20the%20right%20project/#identifying-groups-to-minimalize-initial-resistance","title":"Identifying groups to minimalize initial resistance","text":"Group name Description Canary users Voluntarily test bleeding edge features as soon as they're available. Early adpoters Voluntarily preview releases, considered more refined than code that exposes canary users. Users Consume products after passing through canary and early adopters."},{"location":"DevOps-transformation-journey/Choosing%20the%20right%20project/#kpis","title":"KPIs","text":"<p>Most commonly used KPIs:</p> <ul> <li>Efficiency (Server to Admin Ratio, Application usage &amp; performance),</li> <li>Faster outcomes (Deployment frequency, speed, size, Lead Time),</li> <li>Quality and security (Deployment failure rates, bug report rates, mean time to detection, availabilty),</li> <li>Culture (Employee morale, retention rates).</li> </ul>"},{"location":"DevOps-transformation-journey/Describing%20team%20structures/","title":"Describing team structures","text":""},{"location":"DevOps-transformation-journey/Describing%20team%20structures/#development-strategies","title":"Development strategies","text":""},{"location":"DevOps-transformation-journey/Describing%20team%20structures/#waterfall","title":"Waterfall","text":"<p>The process usually looks like this:</p> <ul> <li>Determining a problem,</li> <li>Analyzing the requirements,</li> <li>Building and testing the required code,</li> <li>The delivery outcome to users.</li> </ul> <p>Follows a sequential order, but lacks the mmechanisms to overcome the problems of useless customer requirements.</p>"},{"location":"DevOps-transformation-journey/Describing%20team%20structures/#agile","title":"Agile","text":"<ul> <li>Constantly emphasizes adaptive planning,</li> <li>Early delivery with continual improvements,</li> <li>Rapid and flexible responses.</li> </ul> <p>Agile manifesto states that:</p> <ol> <li>Development need to favor individuals and interactions over processes and tools.</li> <li>Working software over comprehensive documentation.</li> <li>Customer collaboration over contract negotiation.</li> <li>Respond to changes over following a plan.</li> </ol> <p>Agile software development methods are based on releases and iterations:</p> <ul> <li>One release may consist of several iterations,</li> <li>Each iteration is like a small, independent project,</li> <li>After being estimated and prioritization:</li> <li>Features, bug fixes, enhancements and refactoring width are assigned to a release.</li> <li>And then assigned again to specific iteration within the release, generally on priority basis.</li> <li>At the end of each iteration, there should be tested working code.</li> <li>In each iteration, the team must focus on the outcomes of the previous iteration and learn from them.</li> </ul>"},{"location":"DevOps-transformation-journey/Describing%20team%20structures/#comparison-of-methodologies","title":"Comparison of methodologies","text":"Waterfall Agile Divided into distinct phases. Seperates the project development lifecycle into sprints. Can be rigid. Known for flexibility. All project development phases, such as design, development, and test are completed once. It follows an iterative development approach so that each phase may appear more than once. Define requirements at the start of the project with little change expected. Requirements are expected to change and evolve. Focus on completing the project. Focus on meeting cusotmers' demands."},{"location":"DevOps-transformation-journey/Describing%20team%20structures/#principles-of-agile","title":"Principles of Agile","text":"<ul> <li>Satisfy customer through early and continuous delivery of software.</li> <li>Welcome changing requirements, even late in development. Agile processes harness change for the customer's competitive advantage.</li> <li>Deliver working software frequently, from a couple of months to a couple of weeks, with a preference for a shorter timescale.</li> <li>Businesspeople and developers must work together daily throughout the project.</li> <li>Build projects around motivated individuals. Give them the environment and support they need and trust them to get the job done.</li> <li>The most efficient and effective method of conveying information to and within a development team is face-to-face conversation.</li> <li>Working software is the primary measure of progress.</li> <li>Agile processes promote sustainable development. The sponsors, developers, and users should be able to maintain a constant pace indefinitely.</li> <li>Continuous attention to technical excellence and good design enhances agility.</li> <li>Simplicity - the art of maximizing the amount of work not done - is essential.</li> <li>The best architectures, requirements, and designs emerge from self-organizing teams.</li> <li>The team regularly reflects on how to become more effective, then tunes and adjusts its behavior accordingly.</li> </ul>"},{"location":"DevOps-transformation-journey/Describing%20team%20structures/#team-structure","title":"Team structure","text":"<p>Vertical teams shown better results when using Agile.</p>"},{"location":"DevOps-transformation-journey/Describing%20team%20structures/#horizontal-team","title":"Horizontal team","text":""},{"location":"DevOps-transformation-journey/Describing%20team%20structures/#vertical-team","title":"Vertical team","text":""},{"location":"DevOps-transformation-journey/Describing%20types%20of%20source%20control%20systems/","title":"Describing types of source control systems","text":""},{"location":"DevOps-transformation-journey/Describing%20types%20of%20source%20control%20systems/#centrailized-source-control","title":"Centrailized source control","text":"<ul> <li>Based on the idea that there's a single \"central\" copy of project somewhere (like server).</li> <li>Programmers check in (commit) their changes to central copy.</li> <li>Version control tool can talk to the central copy and retrieve any version they need on the fly.</li> </ul> <p>Most popular centralized version control systems are:</p> <ul> <li>Team Foundation Version Control (TFVC),</li> <li>CVS,</li> <li>Subversion (SVN),</li> <li>Perforce.</li> </ul> Strengths Best used for Easily scales for very large codebases Large integrated codebases. Granual permission control Audit &amp; Access control down to a file level. Permits monitoring of usage Hard to merge file types. Allows exclusive file locking -"},{"location":"DevOps-transformation-journey/Describing%20types%20of%20source%20control%20systems/#typical-centralized-source-control-workflow","title":"Typical centralized source control workflow","text":"<ol> <li>Get the latest changes other people have made from the central server.</li> <li>Make your changes and make sure they work correctly.</li> <li>Check in your changes to the main server so that other programmers can see them.</li> </ol>"},{"location":"DevOps-transformation-journey/Describing%20types%20of%20source%20control%20systems/#distributed-source-control","title":"Distributed source control","text":"<p>Most popular distributed version control systems:</p> <ul> <li>Git,</li> <li>Mercurial,</li> <li>Bazaar.</li> </ul> Strengths Best used for Cross Platform support Small &amp; Modular codebases. An open source friendly code review model via pull requests Evolving through open source. Complete offline support Highly distributed teams. Portable history Teams woring accross platforms. An enthusiastic growing user base Greenfield codebases."},{"location":"DevOps-transformation-journey/Describing%20types%20of%20source%20control%20systems/#advantages-over-centralized-source-control","title":"Advantages over centralized source control","text":"<ul> <li>Doing actions other than pushing and pulling changesets is fast because the tool only needs to access the local storage, not a remote server.</li> <li>Committing new changesets can be done locally without anyone else seeing them. Once you have a group of changesets ready, you can push all of them at once.</li> <li>Everything but pushing and pulling can be done without an internet connection. So, you can work on a plane, and you won't be forced to commit several bug fixes as one large changeset.</li> <li>Since each programmer has a full copy of the project repository, they can share changes with one, or two other people to get feedback before showing the changes to everyone.</li> </ul>"},{"location":"DevOps-transformation-journey/Describing%20types%20of%20source%20control%20systems/#disdvantages-over-centralized-source-control","title":"Disdvantages over centralized source control","text":"<ul> <li>If your project contains many large, binary files that can't be efficiently compressed, the space needed to store all versions of these files can accumulate quickly.</li> <li>If your project has a long history (50,000 changesets or more), downloading the entire history can take an impractical amount of time and disk space.</li> </ul>"},{"location":"DevOps-transformation-journey/Introduction%20to%20DevOps/","title":"Introduction to DevOps","text":""},{"location":"DevOps-transformation-journey/Introduction%20to%20DevOps/#shortening-cycle-time","title":"Shortening cycle time","text":"<ul> <li>Working in smaller batches,</li> <li>Using more automation,</li> <li>Hardening release pipeline,</li> <li>Improving telemetry,</li> <li>Deploying more frequently.</li> </ul> <p>Validated learning - Data gathered during cycle of the project which is factual and actional. The more often you deploy, the more opportunities arise to gather validated data.</p>"},{"location":"DevOps-transformation-journey/Introduction%20to%20DevOps/#exploring-devops","title":"Exploring DevOps","text":"<p>The goal is to shorten the cycle time, to deploy the new code as fast as possible.</p>"},{"location":"DevOps-transformation-journey/Introduction%20to%20DevOps/#some-devops-concepts","title":"Some DevOps concepts","text":"<ol> <li>Continuous Integration drives the ongoing merging and testing of code, leading to an earl finding of defects. Other benefits include less time wasted fighting merge issues and rapid feedback for developers.</li> <li>Continuous Delivery to production and testing environments enables organizations quickly fix bugs.</li> <li>Verison Control enables team to efficiently work on projects.</li> <li>Agile planning is used for project management:</li> <li>Planning and isolating work into sprints</li> <li>Managing team capacity and help teams to adapt quickly</li> <li>Monitoring and Logging to quickly find problems and collect data needed for feedback.</li> <li>Public and Hybrid clouds.</li> <li>Infrastructure as Code makes automation of configuring environments easy.</li> <li>Microservices are used to isolate small use cases into reusable, independent units.</li> <li>Containers are much more lightweight than virtual machines.</li> </ol>"},{"location":"DevOps-transformation-journey/Introduction%20to%20source%20control/","title":"Introduction to source control","text":""},{"location":"DevOps-transformation-journey/Introduction%20to%20source%20control/#what-is-source-control","title":"What is source control?","text":"<p>Allows developers to collaborate on code and track changes. </p>"},{"location":"DevOps-transformation-journey/Introduction%20to%20source%20control/#what-is-version-control","title":"What is version control?","text":"<p>The version control system saves a snapshot of files so that you can review and even roll back to any version of your code with ease. It also helps to resolve conflicts when merging contributions from multiple sources.</p>"},{"location":"DevOps-transformation-journey/Introduction%20to%20source%20control/#benefits-of-source-control","title":"Benefits of source control","text":"<ul> <li>Creating workflows: prevent the chaos of everyone using their development process with different and incompatible tools. Version control systems provide process enforcement and permissions, so everyone stays on the same page.</li> <li>Working with versions: each new version of code has a message describing what has changed.</li> <li>Collaboration: version control makes sure you don't conflict with other changes.</li> <li>Maintaining history of changes: self-explanatory.</li> <li>Automating tasks: control automation saves team time and generate consistent results.</li> </ul>"},{"location":"DevOps-transformation-journey/Introduction%20to%20source%20control/#common-software-development-values","title":"Common software development values","text":"<ul> <li>Reusability.</li> <li>Traceability.</li> <li>Manageability.</li> <li>Efficiency.</li> <li>Collaboration.</li> <li>Learning.</li> </ul>"},{"location":"DevOps-transformation-journey/Introduction%20to%20source%20control/#best-practices-for-source-control","title":"Best practices for source control","text":"<ul> <li>Make small changes: commit early and commit often.</li> <li>Do not commit personal files: SSH keys or PATs may be stealed.</li> <li>Update often and right before pushing to avoid merge conflicts.</li> <li>Verify your code change before pushing it to a repository.</li> <li>Pay close attention to commit messages: as it will tell you why the changes was made.</li> <li>Link code changes to work items: for better traceability.</li> </ul>"},{"location":"DevOps-transformation-journey/Work%20with%20Azure%20Repos%20and%20GitHub/","title":"Work with Azure Repos and GitHub","text":""},{"location":"DevOps-transformation-journey/Work%20with%20Azure%20Repos%20and%20GitHub/#migrating-from-tfvc-to-git","title":"Migrating from TFVC to Git","text":"<ol> <li>Create an empty Git repository.</li> <li>Get-latest from TFS.</li> <li>Copy/reorganize the code into the empty Git repository.</li> <li>Commit and push.</li> </ol> <p>It is also possible to import a TFVC data into Azure Repos using Import in Azure DevOps.</p>"},{"location":"DevOps-transformation-journey/Work%20with%20Azure%20Repos%20and%20GitHub/#migrating-multiple-branches-to-git-from-tfvc","title":"Migrating multiple branches to Git from TFVC","text":"<p>GIT-TFS is an open-source project used to synchronize Git and TFVC repositories.</p> <ol> <li>Run <code>choco install gittfs</code>.</li> <li>Add GIT-TFS folder to path.</li> <li>Clone the repository: <code>git tfs clone http://tfs:8080/tfs/DefaultCollection $/some_project</code>.</li> <li>Enter the repository.</li> </ol>"},{"location":"Development-for-enterprise-DevOps/Collaborate%20with%20PRs%20in%20Azure%20Repos/","title":"Collaborate with PRs in Azure Repos","text":"<p>Once a pull request is sent, interested parties can review the set of changes, discuss potential modifications, and even push follow-up commits if necessary.</p> <p>Once you're done fixing a bug or new feature in a branch, create a new pull request. Add the team members to the pull request so they can review and vote on your changes.</p> <ul> <li>Get your code reviewed.</li> <li>Protect branches with policies.</li> </ul>"},{"location":"Development-for-enterprise-DevOps/Collaborate%20with%20PRs%20in%20Azure%20Repos/#setting-pr-collaboration-in-azure-repos","title":"Setting PR collaboration in Azure Repos","text":"<ol> <li>From Azure DevOps, enter your repository, select the main branch and hit Branch policies.</li> <li>In the policies view, enter your desired branch policies, like number of Approvers.</li> <li>You may want to use the review policy with the comment-resolution policy. It allows you to enforce that the code review comments are resolved before the changes are accepted. The requester can take the feedback from the comment and create a new work item and resolve the changes.</li> <li>Select Check for linked items, because without it, it becomes hard to understand why it was made over time. </li> <li>Select the option to automatically include reviewers when a pull request is raised automatically.</li> </ol>"},{"location":"Development-for-enterprise-DevOps/Exploring%20Git%20hooks/","title":"Exploring Git hooks","text":"<p>Git hooks are a mechanism that allows code to be run before or after certain Git lifecycle events. For example, one could hook into the commit-msg event to validate that the commit message structure follows the recommended format.</p> <p>The hooks can be any executable code, including shell, PowerShell, Python, or other scripts. Or they may be a binary executable. The only criteria are that hooks must be stored in the .git/hooks folder in the repo root. Also, they must be named to match the related events (Git 2.x).</p> <p>Some examples of where you can use hooks to enforce policies, ensure consistency, and control your environment:</p> <ul> <li>In Enforcing preconditions for merging.</li> <li>Verifying work Item ID association in your commit message.</li> <li>Preventing you &amp; your team from committing faulty code.</li> <li>Sending notifications to your team's chat room (Teams, Slack, HipChat, etc.).</li> </ul>"},{"location":"Development-for-enterprise-DevOps/Exploring%20Git%20hooks/#implementing-git-hooks","title":"Implementing Git hooks","text":"<p>Git ships with several sample hook scripts in the repo .git\\hooks directory. Git executes the contents on the particular occasion type they're approached.</p> <p>You can write your custom scripts and put them in that folder. </p>"},{"location":"Development-for-enterprise-DevOps/Identifying%20technical%20debt/","title":"Identifying technical debt","text":""},{"location":"Development-for-enterprise-DevOps/Identifying%20technical%20debt/#five-traits-to-measure-quality-of-code","title":"Five traits to measure quality of code","text":"<ol> <li>Reliability.</li> <li>Maintainabilty.</li> <li>Testability.</li> <li>Portability.</li> <li>Reusability.</li> </ol>"},{"location":"Development-for-enterprise-DevOps/Identifying%20technical%20debt/#quality-related-metrics","title":"Quality-related metrics","text":"<ul> <li>Failed builds percentage: Overall, what percentage of builds are failing?</li> <li>Failed deployments percentage: Overall, what percentage of deployments are failing?</li> <li>Ticket volume: What is the overall volume of customer or bug tickets?</li> <li>Bug bounce percentage: What percentage of customer or bug tickets are reopened?</li> <li>Unplanned work percentage: What percentage of the overall work is unplanned?</li> </ul>"},{"location":"Development-for-enterprise-DevOps/Identifying%20technical%20debt/#technical-debt-sources","title":"Technical debt sources","text":"<ul> <li>Lack of coding style and standards.</li> <li>Lack of or poor design of unit test cases.</li> <li>Ignoring or not understanding object oriented design principles.</li> <li>Monolithic classes and code libraries.</li> <li>Poorly envisioned the use of technology, architecture, and approach. (Forgetting that all system attributes, affecting maintenance, user experience, scalability, and others, need to be considered).</li> <li>Over-engineering code (adding or creating code that isn't required, adding custom code when existing libraries are sufficient, or creating layers or components that aren't needed).</li> <li>Insufficient comments and documentation.</li> <li>Not writing self-documenting code (including class, method, and variable names that are descriptive or indicate intent).</li> <li>Taking shortcuts to meet deadlines.</li> <li>Leaving dead code in place.</li> </ul>"},{"location":"Development-for-enterprise-DevOps/Identifying%20technical%20debt/#quality-tools","title":"Quality tools","text":"<ul> <li>NDepend.</li> <li>Visual Studio marketplace (with keyword Quality).</li> <li>Resharper Code Quality Analysis.</li> </ul>"},{"location":"Development-for-enterprise-DevOps/Managing%20Git%20branches%20and%20workflows/","title":"Managing Git branches and workflows","text":""},{"location":"Development-for-enterprise-DevOps/Managing%20Git%20branches%20and%20workflows/#trunk-based-development","title":"Trunk-based development","text":"<p>The core idea behind the Feature Branch Workflow is that all feature development should take place in a dedicated branch instead of the main branch.</p>"},{"location":"Development-for-enterprise-DevOps/Managing%20Git%20branches%20and%20workflows/#forking-workflow","title":"Forking workflow","text":"<p>Instead of using a single server-side repository to act as the \"central\" codebase, it gives every developer a server-side repository. It means that each contributor has two Git repositories:</p> <ul> <li>A private local one.</li> <li>A public server-side one.</li> </ul>"},{"location":"Development-for-enterprise-DevOps/Managing%20Git%20branches%20and%20workflows/#commands-used-to-set-up-azure-repos","title":"Commands used to set up Azure Repos","text":"<p>Configuring organisation and projcet name:</p> <pre><code>az devops configure --defaults organization=https://dev.azure.com/organization project=\"project name\"\n</code></pre> <p>Creating a PR:</p> <pre><code>az repos pr create --title \"Review Feature-1 before merging to main\" --work-items 38 39 `\n--description \"#Merge feature-1 to main\" `\n--source-branch feature/myFeature-1 --target-branch main `\n--repository myWebApp --open\n</code></pre>"},{"location":"Development-for-enterprise-DevOps/Managing%20Git%20repositories/","title":"Managing Git repositories","text":""},{"location":"Development-for-enterprise-DevOps/Managing%20Git%20repositories/#why-repositories-get-large","title":"Why repositories get large?","text":"<ul> <li>Long history.</li> <li>Large binary files.</li> </ul>"},{"location":"Development-for-enterprise-DevOps/Managing%20Git%20repositories/#workarounds","title":"Workarounds","text":""},{"location":"Development-for-enterprise-DevOps/Managing%20Git%20repositories/#shallow-clone","title":"Shallow clone","text":"<pre><code>git clone --depth [depth] [clone-url]\n</code></pre>"},{"location":"Development-for-enterprise-DevOps/Managing%20Git%20repositories/#vfs-for-git","title":"VFS for Git","text":"<p>VFS for Git helps with large repositories. It requires a Git LFS client. Typical Git commands are unaffected, but the Git LFS works with the standard filesystem to download necessary files in the background when you need files from the server.</p>"},{"location":"Development-for-enterprise-DevOps/Managing%20Git%20repositories/#scalar","title":"Scalar","text":"<p>It achieves by enabling some advanced Git features, such as:</p> <ul> <li>Partial clone: reduces time to get a working repository by not downloading all Git objects right away.</li> <li>Background prefetch: downloads Git object data from all remotes every hour, reducing the time for foreground git fetch calls.</li> <li>Sparse-checkout: limits the size of your working directory.</li> <li>File system monitor: tracks the recently modified files and eliminates the need for Git to scan the entire work tree.</li> <li>Commit-graph: accelerates commit walks and reachability calculations, speeding up commands like git log.</li> <li>Multi-pack-index: enables fast object lookups across many pack files.</li> <li>Incremental repack: Repacks the packed Git data into fewer pack files without disrupting concurrent commands using the multi-pack-index.</li> </ul>"},{"location":"Development-for-enterprise-DevOps/Managing%20Git%20repositories/#purging-repository-data","title":"Purging repository data","text":"<p>If you commit sensitive data (for example, password, key) to Git, it can be removed from history. Two tools are commonly used:</p> <ul> <li>Git filter-repo tool.</li> <li>BFG Repo-Cleaner.</li> </ul>"},{"location":"Development-for-enterprise-DevOps/Managing%20Git%20repositories/#managing-releases-in-github","title":"Managing releases in GitHub","text":"<p>Releases in GitHub are based on Git tags.</p>"},{"location":"Development-for-enterprise-DevOps/Managing%20Git%20repositories/#creating-a-release","title":"Creating a release","text":"<p>To create a release, use the <code>gh</code> release create command. Replace the tag with the desired tag name for the release and follow the interactive prompts.</p> <pre><code>gh release create tag\n</code></pre> <p>To create a prerelease with the specified title and notes.</p> <pre><code>gh release create v1.2.1 --title\n</code></pre>"},{"location":"Development-for-enterprise-DevOps/Planning%20foster%20inner%20source/","title":"Planning foster inner source","text":"<p>Inner source \u2013 sometimes called \"internal open source\" \u2013 brings all the benefits of open-source software development inside your firewall. It opens your software development processes so that your developers can easily collaborate on projects across your company. It uses the same processes that are popular throughout the open-source software communities.</p>"},{"location":"Development-for-enterprise-DevOps/Planning%20foster%20inner%20source/#forking-workflow","title":"Forking workflow","text":"<ol> <li>Create a fork.</li> <li>Clone it locally.</li> <li>Make your changes locally and push them to a branch.</li> <li>Create and complete a PR to upstream.</li> <li>Sync your fork to the latest from upstream.</li> </ol>"},{"location":"Development-for-enterprise-DevOps/Planning%20foster%20inner%20source/#forking-repositories-in-azure-devops","title":"Forking repositories in Azure DevOps","text":"<ol> <li>Navigate to the repository to fork and choose fork.</li> <li>Specify a name and choose the project where you want the fork to be created. If the repository contains many topic branches, we recommend you fork only the default branch.</li> <li>Choose the ellipsis, then Fork to create the fork.</li> </ol>"},{"location":"Development-for-enterprise-DevOps/Structuring%20Git%20Repo/","title":"Structuring Git Repo","text":""},{"location":"Development-for-enterprise-DevOps/Structuring%20Git%20Repo/#implementing-a-changelog","title":"Implementing a changelog","text":"<p>The typical breakdown is to separate a list of versions, and then within each version, show:</p> <ul> <li>Added features.</li> <li>Modified/Improved features.</li> <li>Deleted features.</li> </ul>"},{"location":"Development-for-enterprise-DevOps/Structuring%20Git%20Repo/#changelog-using-git","title":"Changelog using git","text":"<pre><code>git log [options] vX.X.X..vX.X.Y | helper-script &gt; projectchangelogs/X.X.Y\n</code></pre>"},{"location":"Development-for-enterprise-DevOps/Structuring%20Git%20Repo/#using-github-changelog-generator","title":"Using GitHub Changelog Generator","text":"<pre><code>$ github_changelog_generator -u github-changelog-generator -p TimerTrend-3.0\n</code></pre>"},{"location":"Implement-CI-with-Azure-Pipelines-and-GitHub-Actions/Continuous%20Integration%20with%20GitHub%20Actions/","title":"Continuous Integration with GitHub Actions","text":""},{"location":"Implement-CI-with-Azure-Pipelines-and-GitHub-Actions/Continuous%20Integration%20with%20GitHub%20Actions/#basic-building-blocks-of-workflows","title":"Basic building blocks of workflows","text":"<pre><code>name: dotnet Build\n\non: [push]\n\njobs:\n    build:\n        runs-on: ubuntu-latest\n        strategy:\n            matrix:\n                node-version: [10.x]\n        steps:\n\n        - uses: actions/checkout@main\n        - uses: actions/setup-dotnet@v1\n            with:\n                dotnet-version: '3.1.x'\n\n        - run: dotnet build awesomeproject\n</code></pre> <p>Legend:</p> <ul> <li>On: Specifies what will occur when code is pushed.</li> <li>Jobs: There's a single job called build.</li> <li>Strategy: It's being used to specify the Node.js version.</li> <li>Steps: Are doing a checkout of the code and setting up dotnet.</li> <li>Run: Is building the code.</li> </ul>"},{"location":"Implement-CI-with-Azure-Pipelines-and-GitHub-Actions/Continuous%20Integration%20with%20GitHub%20Actions/#environment-variables","title":"Environment variables","text":"<p>GitHub provides a series of built-in environment variables. It all has a <code>GITHUB_</code> prefix.</p> <ul> <li>GITHUB_WORKFLOW is the name of the workflow.</li> <li>GITHUB_ACTION is the unique identifier for the action.</li> <li>GITHUB_REPOSITORY is the name of the repository (but also includes the name of the owner in owner/repo format).</li> </ul> <p>Variables are set in the YAML workflow files. They're passed to the actions that are in the step.</p>"},{"location":"Implement-CI-with-Azure-Pipelines-and-GitHub-Actions/Continuous%20Integration%20with%20GitHub%20Actions/#sharing-artifacts-between-jobs","title":"Sharing artifacts between jobs","text":"<p>The most common ways to do it are by using the <code>upload-artifact</code> and <code>download-artifact</code> actions.</p>"},{"location":"Implement-CI-with-Azure-Pipelines-and-GitHub-Actions/Continuous%20Integration%20with%20GitHub%20Actions/#upload-artifact","title":"<code>upload-artifact</code>","text":"<p>This action can upload one or more files from your workflow to be shared between jobs.</p> <pre><code>- uses: actions/upload-artifact\n  with:\n    name: harness-build-log\n    path: bin/output/logs/harness.log\n</code></pre>"},{"location":"Implement-CI-with-Azure-Pipelines-and-GitHub-Actions/Continuous%20Integration%20with%20GitHub%20Actions/#download-artifact","title":"<code>download-artifact</code>","text":"<p>There's a corresponding action for downloading (or retrieving) artifacts.</p> <pre><code>- uses: actions/download-artifact\n  with:\n    name: harness-build-log\n</code></pre>"},{"location":"Implement-CI-with-Azure-Pipelines-and-GitHub-Actions/Continuous%20Integration%20with%20GitHub%20Actions/#artifact-retention","title":"Artifact retention","text":"<p>A default retention period can be set for the repository, organization, or enterprise. You can set a custom retention period when uploading, but it can't exceed the defaults for the repository, organization, or enterprise.</p> <pre><code>- uses: actions/upload-artifact\n  with:\n    name: harness-build-log\n    path: bin/output/logs/harness.log\n    retention-days: 12\n</code></pre>"},{"location":"Implement-CI-with-Azure-Pipelines-and-GitHub-Actions/Continuous%20Integration%20with%20GitHub%20Actions/#workflow-badges","title":"Workflow badges","text":"<p>Badges can be used to show the status of a workflow within a repository. They show if a workflow is currently passing or failing. While they can appear in several locations, they typically get added to the README.md file for the repository.</p> <p>Badges are added by using URLs. The URLs are formed as follows:</p> <p><code>https://github.com/&lt;OWNER&gt;/&lt;REPOSITORY&gt;/actions/workflows/&lt;WORKFLOW_FILE&gt;/badge.svg</code></p>"},{"location":"Implement-CI-with-Azure-Pipelines-and-GitHub-Actions/Continuous%20Integration%20with%20GitHub%20Actions/#best-practices-for-creating-actions","title":"Best practices for creating actions","text":"<ul> <li>Create chainable actions. Don't create large monolithic actions. Instead, create smaller functional actions that can be chained together.</li> <li>Version your actions like other code. Others might take dependencies on various versions of your actions. Allow them to specify versions.</li> <li>Provide the latest label. If others are happy to use the latest version of your action, make sure you provide the latest label that they can specify to get it.</li> <li>Add appropriate documentation. As with other codes, documentation helps others use your actions and can help avoid surprises about how they function.</li> <li>Add details <code>action.yml</code> metadata. At the root of your action, you'll have an <code>action.yml</code> file. Ensure it has been populated with author, icon, expected inputs, and outputs.</li> <li>Consider contributing to the marketplace. It's easier for us to work with actions when we all contribute to the marketplace. Help to avoid people needing to relearn the same issues endlessly.</li> </ul>"},{"location":"Implement-CI-with-Azure-Pipelines-and-GitHub-Actions/Continuous%20Integration%20with%20GitHub%20Actions/#marking-releases-with-git-tags","title":"Marking releases with Git tags","text":"<p>Releases are software iterations that can be packed for release. In Git, releases are based on Git tags. These tags mark a point in the history of the repository. Tags are commonly assigned as releases are created.</p>"},{"location":"Implement-CI-with-Azure-Pipelines-and-GitHub-Actions/Continuous%20Integration%20with%20GitHub%20Actions/#encrypted-secrets","title":"Encrypted secrets","text":"<p>Secrets are similar to environment variables but encrypted. They can be created at two levels:</p> <ul> <li>Repository.</li> <li>Organization.</li> </ul> <p>If secrets are created at the organization level, access policies can limit the repositories that can use them. You can create secrets in Settings -&gt; Secrets -&gt; New Secret.</p>"},{"location":"Implement-CI-with-Azure-Pipelines-and-GitHub-Actions/Continuous%20Integration%20with%20GitHub%20Actions/#using-secrets-in-workflow","title":"Using secrets in workflow","text":"<p>Secrets aren't passed automatically to the runners when workflows are executed. Instead, when you include an action that requires access to a secret, you use the secrets context to provide it.</p> <pre><code>steps:\n\n  - name: Test Database Connectivity\n    with:\n      db_username: ${{ secrets.DBUserName }}\n      db_password: ${{ secrets.DBPassword }}\n</code></pre> <p>Workflows can use up to 100 secrets. Their size is limited to 64kB of data.</p>"},{"location":"Implement-CI-with-Azure-Pipelines-and-GitHub-Actions/Continuous%20Integration/","title":"Continuous Integration","text":"<p>Continuous integration (CI) is the process of automating the build and testing of code every time a team member commits changes to version control.</p>"},{"location":"Implement-CI-with-Azure-Pipelines-and-GitHub-Actions/Continuous%20Integration/#pillars-of-continuous-integration","title":"Pillars of Continuous Integration","text":"<p>Version Control:</p> <ul> <li>Git.</li> <li>Apache Subversion.</li> <li>Team Foundation Version Control.</li> </ul> <p>Package Management:</p> <ul> <li>NuGet.</li> <li>npm.</li> <li>Chocolatey.</li> <li>HomeBrew.</li> <li>RPM.</li> </ul> <p>CI System:</p> <ul> <li>Azure DevOps.</li> <li>TeamCity.</li> <li>Jenkins.</li> </ul> <p>Automated Build Process:</p> <ul> <li>Apache Ant.</li> <li>NAnt2.</li> <li>Gradle.</li> </ul>"},{"location":"Implement-CI-with-Azure-Pipelines-and-GitHub-Actions/Continuous%20Integration/#benefits-of-continuous-integration","title":"Benefits of Continuous Integration","text":"<ul> <li>Improving code quality based on rapid feedback.</li> <li>Triggering automated testing for every code change.</li> <li>Reducing build times for quick feedback and early detection of problems (risk reduction).</li> <li>Better managing technical debt and conducting code analysis.</li> <li>Reducing long, complex, and bug-inducing merges.</li> <li>Increasing confidence in codebase health long before production deployment.</li> </ul>"},{"location":"Implement-CI-with-Azure-Pipelines-and-GitHub-Actions/Continuous%20Integration/#lab-address","title":"Lab address","text":"<p>https://aka.ms/az-400-enable-continuous-integration-with-azure-pipelines</p>"},{"location":"Implement-CI-with-Azure-Pipelines-and-GitHub-Actions/Designing%20a%20container%20build%20strategy/","title":"Designing a container build strategy","text":""},{"location":"Implement-CI-with-Azure-Pipelines-and-GitHub-Actions/Designing%20a%20container%20build%20strategy/#whats-the-difference-between-containers-and-virtualization","title":"What's the difference between containers and virtualization?","text":"<p>Containers and VMs are similar in their goals: to isolate an application and its dependencies into a self-contained unit that can run anywhere. They remove the need for physical hardware, allowing for:</p> <ul> <li>More efficient use of computing resources and enerfy consumption.</li> <li>Cost-effectiveness.</li> </ul> <p>The main difference between containers and VMs is in their architectural approach.</p>"},{"location":"Implement-CI-with-Azure-Pipelines-and-GitHub-Actions/Designing%20a%20container%20build%20strategy/#virtual-machines","title":"Virtual machines","text":"<p>A VM is essentially an emulation of a real computer that executes programs like a real computer. VMs run on top of a physical machine using a hypervisor. VMs package up the virtual hardware, a kernel (OS), and user space for each new VM.</p> <p></p>"},{"location":"Implement-CI-with-Azure-Pipelines-and-GitHub-Actions/Designing%20a%20container%20build%20strategy/#containers","title":"Containers","text":"<p>Unlike a VM, which provides hardware virtualization, a container provides operating-system-level virtualization by abstracting the user space.</p> <p>This diagram shows that containers package up just the user space, not the kernel or virtual hardware like a VM does. Each container gets its isolated user space to allow multiple containers to run on a single host machine. We can see that all the operating system-level architecture is being shared across containers. The only parts that are created from scratch are the bins and libs. It's what makes containers so lightweight.</p> <p></p>"},{"location":"Implement-CI-with-Azure-Pipelines-and-GitHub-Actions/Designing%20a%20container%20build%20strategy/#useful-docker-commands","title":"Useful Docker commands","text":"<ul> <li><code>docker build</code> - You create an image by executing a Dockerfile.</li> <li><code>docker pull</code> - You retrieve the image, likely from a container registry.</li> <li><code>docker run</code> - You execute the container. An instance is created of the image.</li> </ul>"},{"location":"Implement-CI-with-Azure-Pipelines-and-GitHub-Actions/Designing%20a%20container%20build%20strategy/#dockerfile-core-concepts","title":"Dockerfile core concepts","text":"<p>Dockerfiles are text files that contain the commands needed by docker build to assemble an image.</p> <pre><code>FROM ubuntu\nLABEL maintainer=\"johndoe@contoso.com\"\nADD appsetup /\nRUN /bin/bash -c 'source $HOME/.bashrc; echo $HOME'\nCMD [\"echo\", \"Hello World from within the container\"]\n</code></pre> <p>The first line refers to the parent image based on which this new image will be based. Generally, all images will be based on another existing image. In this case, the Ubuntu image would be retrieved from either a local cache or from DockerHub. An image that doesn't have a parent is called a base image. In that rare case, the <code>FROM</code> line can be omitted, or <code>FROM scratch</code> can be used instead.</p> <p>The second line indicates the email address of the person who maintains this file. Previously, there was a MAINTAINER command, but that has been deprecated and replaced by a label.</p> <p>The third line adds a file to the root folder of the image. It can also add an executable.</p> <p>The fourth and fifth lines are part of a RUN command. </p> <p>The <code>RUN</code> command is run when the <code>docker build</code> creates the image. It's used to configure items within the image. By comparison, the last line represents a command that will be executed when a new container is created from the image; it's run after container creation.</p>"},{"location":"Implement-CI-with-Azure-Pipelines-and-GitHub-Actions/Designing%20a%20container%20build%20strategy/#multistage-dockerfiles","title":"Multistage Dockerfiles","text":"<p>Multi-stage builds give the benefits of the builder pattern without the hassle of maintaining three separate files.</p> <pre><code>FROM mcr.microsoft.com/dotnet/core/aspnetcore:3.1 AS base\nWORKDIR /app\nEXPOSE 80\nEXPOSE 443\n\nFROM mcr.microsoft.com/dotnet/core/sdk:3.1 AS build\nWORKDIR /src\nCOPY [\"WebApplication1.csproj\", \"\"]\nRUN dotnet restore \"./WebApplication1.csproj\"\nCOPY . .\nWORKDIR \"/src/.\"\nRUN dotnet build \"WebApplication1.csproj\" -c Release -o /app/build\n\nFROM build AS publish\nRUN dotnet publish \"WebApplication1.csproj\" -c Release -o /app/publish\n\nFROM base AS final\nWORKDIR /app\nCOPY --from=publish /app/publish .\nENTRYPOINT [\"dotnet\", \"WebApplication1.dll\"]\n</code></pre> <p>Description:</p> <p><code>FROM build AS publish</code></p> <p><code>build</code> isn't an image pulled from a registry. It's the image we defined in stage 2, where we named the result of our-build (SDK) image \"builder\". <code>docker build</code> will create a named image we can later reference.</p> <p>We can also copy the output from one image to another. It's the real power to compile our code with one base SDK image (<code>mcr.microsoft.com/dotnet/core/sdk:3.1</code>) while creating a production image based on an optimized runtime image (<code>mcr.microsoft.com/dotnet/core/aspnet:3.1</code>). </p> <p><code>COPY --from=publish /app/publish .</code></p> <p>It takes the <code>/app/publish</code> directory from the published image and copies it to the working directory of the production image.</p>"},{"location":"Implement-CI-with-Azure-Pipelines-and-GitHub-Actions/Designing%20a%20container%20build%20strategy/#breakdown-of-the-example","title":"Breakdown of the example","text":"<p>The first stage provides the base of our optimized runtime image. Notice it derives from <code>mcr.microsoft.com/dotnet/core/aspnet:3.1</code>. We would specify extra production configurations, such as registry configurations, MSIexec of other components. You would hand off any of those environment configurations to your ops folks to prepare the VM.</p> <p>The second stage is our build environment.<code>mcr.microsoft.com/dotnet/core/sdk:3.1</code> This includes everything we need to compile our code. From here, we have compiled binaries we can publish or test\u2014more on testing in a moment.</p> <p>The third stage derives from our build stage. It takes the compiled output and \"publishes\" them in .NET terms. Publish means taking all the output required to deploy your <code>app/publish/service/component</code> and placing it in a single directory. It would include your compiled binaries, graphics (images), JavaScript, and so on.</p> <p>The fourth stage takes the published output and places it in the optimized image we defined in the first stage.</p>"},{"location":"Implement-CI-with-Azure-Pipelines-and-GitHub-Actions/Designing%20a%20container%20build%20strategy/#multistage-build-considerations","title":"Multistage build considerations","text":"<ul> <li>Adopt container modularity: Try to avoid creating overly complex container images that couple together several applications. Instead, use multiple containers and try to keep each container to a single purpose.</li> <li>Avoid unnecessary packages: To help minimize image sizes, it's also essential to avoid including packages that you suspect might be needed but aren't yet sure if they're required.</li> <li>Choose an appropriate base: While optimizing the contents of your Dockerfiles is essential, it's also crucial to choose the appropriate parent (base) image. Start with an image that only contains packages that are required.</li> <li>Avoid including application data: While application data can be stored in the container, it will make your images more prominent.</li> </ul>"},{"location":"Implement-CI-with-Azure-Pipelines-and-GitHub-Actions/Designing%20a%20container%20build%20strategy/#azure-continer-oriented-services","title":"Azure continer-oriented services","text":""},{"location":"Implement-CI-with-Azure-Pipelines-and-GitHub-Actions/Designing%20a%20container%20build%20strategy/#azure-container-instances-aci","title":"Azure Container Instances (ACI)","text":"<p>Running workloads in ACI allows you to create your applications rather than provisioning and managing the infrastructure that will run the applications. ACIs are simple and fast to deploy, and when you're using them, you gain the security of hypervisor isolation for each container group. It ensures that your containers aren't sharing an operating system kernel with other containers.</p>"},{"location":"Implement-CI-with-Azure-Pipelines-and-GitHub-Actions/Designing%20a%20container%20build%20strategy/#azure-kubernetes-service-aks","title":"Azure Kubernetes Service (AKS)","text":"<p>This service lets you quickly deploy and manage Kubernetes, to scale and run applications while maintaining overall solid security.</p>"},{"location":"Implement-CI-with-Azure-Pipelines-and-GitHub-Actions/Designing%20a%20container%20build%20strategy/#azure-container-registry-acr","title":"Azure Container Registry (ACR)","text":"<p>This service lets you store and manage container images in a central registry. It provides you with a Docker private registry as a first-class Azure resource. All container deployments, including DC/OS, Docker Swarm, and Kubernetes, are supported. The registry is integrated with other Azure services such as the App Service, Batch, Service Fabric, and others.</p>"},{"location":"Implement-CI-with-Azure-Pipelines-and-GitHub-Actions/Designing%20a%20container%20build%20strategy/#azure-container-apps","title":"Azure Container Apps","text":"<p>Azure Container Apps allows you to build and deploy modern apps and microservices using serverless containers. It deploys containerized apps without managing complex infrastructure. You can write code using your preferred programming language or framework and build microservices with full support for Distributed Application Runtime (Dapr). Scale dynamically based on HTTP traffic or events powered by Kubernetes Event-Driven Autoscaling (KEDA).</p>"},{"location":"Implement-CI-with-Azure-Pipelines-and-GitHub-Actions/Designing%20a%20container%20build%20strategy/#azure-app-service","title":"Azure App Service","text":"<p>Azure Web Apps provides a managed service for both Windows and Linux-based web applications and provides the ability to deploy and run containerized applications for both platforms. It provides autoscaling and load balancing options and is easy to integrate with Azure DevOps.</p>"},{"location":"Implement-CI-with-Azure-Pipelines-and-GitHub-Actions/Designing%20a%20container%20build%20strategy/#lab-address","title":"Lab address","text":"<p>https://aka.ms/az-400-deploy-docker-containers-to-azure-app-service-web-apps</p>"},{"location":"Implement-CI-with-Azure-Pipelines-and-GitHub-Actions/Exploring%20Azure%20Pipelines/","title":"Exploring Azure Pipelines","text":""},{"location":"Implement-CI-with-Azure-Pipelines-and-GitHub-Actions/Exploring%20Azure%20Pipelines/#description","title":"Description","text":"<ul> <li>Language: nearly every major language, like Python, Go, Java, PHP, C#, etc.</li> <li>Version control systems: GitHub, GitLab, Azure Repos, Bitbucket, Subversion.</li> <li>Deployment targets: Container registries, VMs, any on-premises or public cloud like Azure, AWS or GCP.</li> <li>Package formats: NuGet, npm, Maven are built-in, you may also use any other package management repository of choice.</li> </ul>"},{"location":"Implement-CI-with-Azure-Pipelines-and-GitHub-Actions/Exploring%20Azure%20Pipelines/#cicd-in-projects","title":"CI/CD in projects","text":"Continuous Integration (CI) Continuous Delivery (CD) Increase code coverage. Automatically deploy code to production. Build faster by splitting test and build runs. Ensure deployment targets have the latest code. Automatically ensure you don't ship broken code. Ensure deployment targets have the latest code. Run test continually. -"},{"location":"Implement-CI-with-Azure-Pipelines-and-GitHub-Actions/Exploring%20Azure%20Pipelines/#azure-pipelines-key-terms","title":"Azure Pipelines key terms","text":"<ul> <li>Agent: installable software that runs a build or deployment job.</li> <li>Artifact: collection of files or packages published by a build. Artifacts are made available for the tasks, such as distribution or deployment.</li> <li>Build: one execution of a pipeline. It collects logs associated with running the steps anf the test results.</li> <li>Deployment target: VM, container, web app, or any service used to host the developed application.</li> <li>Job: a build contains one or more jobs. Most jobs run on an agent. A job represents an execution boundary of a set of steps.</li> <li>Stage: the primary division in pipeline: \"build the app\", \"run integration tests\" and \"deploy to user acceptance testing\" are good examples of stages.</li> <li>Task: a building block of a pipeline.Each task runs a specific job in the pipeline.</li> <li>Trigger: tells the pipeline when to run.</li> </ul>"},{"location":"Implement-CI-with-Azure-Pipelines-and-GitHub-Actions/GitHub%20Actions/","title":"GitHub Actions","text":"<p>Actions are the mechanism used to provide workflow automation within the GitHub environment. They are executed on runners either GitHub or self-hosted.</p> <p></p> <p>Workflows are written in YAML and live within a GitHub repository at the place .github/workflows.</p> <pre><code># .github/workflows/build.yml\nname: Node Build.\n\non: [push]\n\njobs:\n    mainbuild:\n\n        runs-on: ${{ matrix.os }}\n\n    strategy:\n        matrix:\n            node-version: [12.x]\n            os: [windows-latest]\n\n    steps:\n\n    - uses: actions/checkout@v1\n    - name: Run node.js on latest Windows.\n      uses: actions/setup-node@v1\n      with:\n        node-version: ${{ matrix.node-version }}\n\n    - name: Install NPM and build.\n      run: |\n        npm ci\n        npm run build\n</code></pre> <p>Legend:</p> <ul> <li><code>Name</code>: is the name of the workflow. It's optional but is highly recommended. It appears in several places within the GitHub UI.</li> <li><code>On</code>: is the event or list of events that will trigger the workflow.</li> <li><code>Jobs</code>: is the list of jobs to be executed. Workflows can contain one or more jobs.</li> <li><code>Runs-on</code>: tells Actions which runner to use.</li> <li><code>Steps</code>: It's the list of steps for the job. Steps within a job execute on the same runner.</li> <li><code>Uses</code>: tells Actions, which predefined action needs to be retrieved. For example, you might have an action that installs node.js.</li> <li><code>Run</code>: tells the job to execute a command on the runner. For example, you might execute an NPM command.</li> </ul>"},{"location":"Implement-CI-with-Azure-Pipelines-and-GitHub-Actions/GitHub%20Actions/#events","title":"Events","text":"<p>Events may be triggered by:</p> <ul> <li>Scheduled events (using cron schedule).</li> <li>Code events.</li> <li>Manual events.</li> <li>Webhook events.</li> <li>External events.</li> </ul>"},{"location":"Implement-CI-with-Azure-Pipelines-and-GitHub-Actions/GitHub%20Actions/#jobs","title":"Jobs","text":"<p>Workflows contain one or more jobs. A job is a set of steps that will be run in order on a runner. Steps within a job execute on the same runner and share the same filesystem. The logs produced by jobs are searchable, and artifacts produced can be saved.</p> <p>By default, if a workflow contains multiple jobs, they run in parallel.</p> <pre><code>jobs:\n  startup:\n    runs-on: ubuntu-latest\n    steps:\n\n      - run: ./setup_server_configuration.sh\n  build:\n    steps:\n\n      - run: ./build_new_server.sh\n</code></pre> <p>Defining dependencies between jobs using <code>needs</code> keyword:</p> <pre><code>jobs:\n  startup:\n    runs-on: ubuntu-latest\n    steps:\n\n      - run: ./setup_server_configuration.sh\n  build:\n    needs: startup\n    steps:\n\n      - run: ./build_new_server.sh\n</code></pre>"},{"location":"Implement-CI-with-Azure-Pipelines-and-GitHub-Actions/GitHub%20Actions/#runners","title":"Runners","text":"<p>When you execute jobs, the steps execute on a Runner. The steps can be the execution of a shell script or the execution of a predefined Action. GitHub provides several hosted runners to avoid you needing to spin up your infrastructure to run actions. Now, the maximum duration of a job is 6 hours, and for a workflow is 72 hours. GitHub provides only Linux-based containers.</p> <p>If this doesn't meet your requirements, you may use a self-hosted agent.</p> <p>Self-hosted runners can be added at different levels within an enterprise: Organizational-level (multiple repositories in an organization), Enterprise-level (multiple organizations across an enterprise), Repository-level (single repository).</p>"},{"location":"Implement-CI-with-Azure-Pipelines-and-GitHub-Actions/Integrating%20with%20Azure%20Pipelines/","title":"Integrating with Azure Pipelines","text":""},{"location":"Implement-CI-with-Azure-Pipelines-and-GitHub-Actions/Integrating%20with%20Azure%20Pipelines/#steps","title":"Steps","text":"<p>A step is a linear sequence of operations that make up a job. Each step runs its process on an agent and accesses the pipeline workspace on a local hard drive. This behavior means environment variables aren't preserved between steps, but file system changes are.</p> <pre><code>steps:\n\n- script: echo This run in the default shell on any machine\n- bash: |\n    echo This multiline script always runs in Bash.\n    echo Even on Windows machines!\n\n- pwsh: |\n    Write-Host \"This multiline script always runs in PowerShell Core.\"\n    Write-Host \"Even on non-Windows machines!\"\n</code></pre>"},{"location":"Implement-CI-with-Azure-Pipelines-and-GitHub-Actions/Integrating%20with%20Azure%20Pipelines/#tasks","title":"Tasks","text":"<p>Tasks are the building blocks of a pipeline. There's a catalog of tasks available to choose from.</p> <pre><code>steps:\n\n- task: VSBuild@1\n  displayName: Build\n  timeoutInMinutes: 120\n  inputs:\n    solution: '**\\*.sln'\n</code></pre>"},{"location":"Implement-CI-with-Azure-Pipelines-and-GitHub-Actions/Integrating%20with%20Azure%20Pipelines/#hello-world-pipeline","title":"Hello World pipeline","text":"<p>Steps are the actual \"things\" that execute in the order specified in the job. Each step is a task: out-of-the-box (OOB) tasks come with Azure DevOps. Many have aliases and tasks installed on your Azure DevOps organization via the marketplace.</p> <pre><code>name: 1.0$(Rev:.r)\n\n# simplified trigger (implied branch)\ntrigger:\n\n- main\n\n# equivalents trigger\n# trigger:\n#  branches:\n#    include:\n#    - main\n\nvariables:\n  name: John\n\npool:\n  vmImage: ubuntu-latest\n\njobs:\n\n- job: helloworld\n  steps:\n    - checkout: self\n    - script: echo \"Hello, $(name)\"\n</code></pre> <p>Legend:</p> <ul> <li>Name: If it's skipped, a date-based name is generated automatically.</li> <li>Trigger: There's an implicit \"trigger on every commit to any path from any branch in this repo.\" without any explicit trigger.</li> <li>Variables: \"Inline\" variables. </li> <li>Job: every pipeline must have at least one job.</li> <li>Pool: you configure which pool (queue) the job must run on.</li> <li>Checkout: the \"checkout: self\" tells the job which repository (or repositories if there are multiple checkouts) to check out for this job.</li> <li>Steps: the actual tasks that need to be executed: in this case, a \"script\" task (the script is an alias) that can run inline scripts.</li> </ul>"},{"location":"Implement-CI-with-Azure-Pipelines-and-GitHub-Actions/Integrating%20with%20Azure%20Pipelines/#dependencies","title":"Dependencies","text":"<p>Consider this pipeline:</p> <pre><code>jobs:\n\n- job: A\n  steps:\n  # steps omitted for brevity\n\n\n- job: B\n  steps:\n  # steps omitted for brevity\n</code></pre> <p>Because no <code>dependsOn</code> was specified, the jobs will run sequentially: first A and then B. To have both jobs run in parallel, we add dependsOn: <code>[] to job B</code>:</p> <pre><code>jobs:\n\n- job: A\n  steps:\n  # steps omitted for brevity\n\n\n- job: B\n  dependsOn: [] # This removes the implicit dependency on the previous stage and causes this to run in parallel.\n  steps:\n  # steps omitted for brevity\n</code></pre>"},{"location":"Implement-CI-with-Azure-Pipelines-and-GitHub-Actions/Integrating%20with%20Azure%20Pipelines/#resources","title":"Resources","text":"<p>Resources let you reference:</p> <ul> <li>Other repositories.</li> <li>Pipelines.</li> <li>Builds (classic builds).</li> <li>Containers (for container jobs).</li> <li>Packages.</li> </ul> <p>To reference code in another repository, refer to it in the resources section, then reference it via its alias in checkout step:</p> <pre><code>resources:\n  repositories:\n\n  - repository: appcode\n    type: git\n    name: otherRepo\n\nsteps:\n\n- checkout: appcode\n</code></pre>"},{"location":"Implement-CI-with-Azure-Pipelines-and-GitHub-Actions/Integrating%20with%20Azure%20Pipelines/#more-about-variables","title":"More about variables","text":"<p>To dereference a variable, wrap the key in <code>$()</code>:</p> <pre><code>variables:\n  name: John\nsteps:\n\n- script: echo \"Hello, $(name)!\"\n</code></pre>"},{"location":"Implement-CI-with-Azure-Pipelines-and-GitHub-Actions/Integrating%20with%20Azure%20Pipelines/#pipeline-structure","title":"Pipeline structure","text":"<p>Stages are the primary divisions in a pipeline. The stages \"Build this app,\" \"Run these tests,\" and \"Deploy to preproduction\" are good examples. A stage is one or more jobs, units of work assignable to the same machine.</p> <p>Example hierarchy of a <code>.yaml</code> file:</p> <ul> <li>Pipeline<ul> <li>Stage A<ul> <li>Job 1<ul> <li>Step 1.1</li> <li>Step 1.2</li> <li>...</li> </ul> </li> <li>Job 2<ul> <li>Step 2.1</li> <li>Step 2.2</li> <li>...</li> </ul> </li> </ul> </li> <li>Stage B<ul> <li>... </li> </ul> </li> </ul> </li> </ul>"},{"location":"Implement-CI-with-Azure-Pipelines-and-GitHub-Actions/Integrating%20with%20Azure%20Pipelines/#schema-for-pipeline","title":"Schema for pipeline","text":"<pre><code>name: string  # build numbering format\nresources:\n  pipelines: [ pipelineResource ]\n  containers: [ containerResource ]\n  repositories: [ repositoryResource ]\nvariables: # several syntaxes\ntrigger: trigger\npr: pr\nstages: [ stage | templateReference ]\n</code></pre> <p>If you have a single-stage pipeline, you can omit the stages keyword:</p> <pre><code># ... other pipeline-level keywords\njobs: [ job | templateReference ]\n</code></pre> <p>If you've a single-stage and a single job, you can omit the stages and jobs keywords and directly specify the steps keyword:</p> <pre><code># ... other pipeline-level keywords\nsteps: [ script | bash | pwsh | powershell | checkout | task | templateReference ]\n</code></pre>"},{"location":"Implement-CI-with-Azure-Pipelines-and-GitHub-Actions/Integrating%20with%20Azure%20Pipelines/#stages","title":"Stages","text":"<p>A stage is a collection of related jobs. By default, stages run sequentially. Each stage starts only after the preceding stage is complete. Use approval checks to control when a stage should run manually. </p> <p>This example runs three stages, one after another. The middle stage runs two parallel jobs:</p> <pre><code>stages:\n\n- stage: Build\n  jobs:\n\n  - job: BuildJob\n    steps:\n\n    - script: echo Building!\n- stage: Test\n  dependsOn: Build\n  jobs:\n\n  - job: TestOnWindows\n    steps:\n\n    - script: echo Testing on Windows!\n  - job: TestOnLinux\n    steps:\n\n    - script: echo Testing on Linux!\n- stage: Deploy\n  dependsOn: Test\n  jobs:\n\n  - job: Deploy\n    steps:\n\n    - script: echo Deploying the code!\n</code></pre>"},{"location":"Implement-CI-with-Azure-Pipelines-and-GitHub-Actions/Integrating%20with%20Azure%20Pipelines/#jobs","title":"Jobs","text":"<p>A job is a collection of steps run by an agent or on a server. Jobs can run conditionally and might depend on previous jobs.</p> <pre><code>jobs:\n\n- job: MyJob\n  displayName: My First Job\n  continueOnError: true\n  workspace:\n    clean: outputs\n  steps:\n\n  - script: echo My first job\n</code></pre>"},{"location":"Implement-CI-with-Azure-Pipelines-and-GitHub-Actions/Integrating%20with%20Azure%20Pipelines/#deployment-strategies","title":"Deployment strategies","text":""},{"location":"Implement-CI-with-Azure-Pipelines-and-GitHub-Actions/Integrating%20with%20Azure%20Pipelines/#runonce","title":"runOnce","text":"<p><code>runOnce</code> is the simplest deployment strategy wherein all the lifecycle hooks, namely <code>preDeploy</code>, <code>deploy</code>, <code>routeTraffic</code>, and <code>postRouteTraffic</code>, are executed once. Then, either <code>on: success</code> or <code>on: failure</code> is executed.</p> <pre><code>strategy: \n    runOnce:\n      preDeploy:        \n        pool: [ server | pool ] # See pool schema.        \n        steps:\n        - script: [ script | bash | pwsh | powershell | checkout | task | templateReference ]\n      deploy:          \n        pool: [ server | pool ] # See pool schema.        \n        steps:\n        ...\n      routeTraffic:         \n        pool: [ server | pool ]         \n        steps:\n        ...        \n      postRouteTraffic:          \n        pool: [ server | pool ]        \n        steps:\n        ...\n      on:\n        failure:         \n          pool: [ server | pool ]           \n          steps:\n          ...\n        success:          \n          pool: [ server | pool ]           \n          steps:\n          ...\n</code></pre>"},{"location":"Implement-CI-with-Azure-Pipelines-and-GitHub-Actions/Integrating%20with%20Azure%20Pipelines/#rolling","title":"Rolling","text":"<p>A <code>rolling</code> deployment replaces instances of the previous version of an application with instances of the new version. It can be configured by specifying the keyword <code>rolling:</code> under the <code>strategy: node</code>.</p> <pre><code>strategy:\n    rolling:\n        maxParallel: [ number or percentage as x% ]\n        preDeploy:       \n            steps:\n            - script: [ script | bash | pwsh | powershell | checkout | task | templateReference ]\n        deploy:         \n            steps:\n            ...\n        routeTraffic:       \n            steps:\n            ...       \n        postRouteTraffic:         \n            steps:\n            ...\n        on:\n            failure:       \n                steps:\n                ...\n            success:         \n                steps:\n                ...\n</code></pre>"},{"location":"Implement-CI-with-Azure-Pipelines-and-GitHub-Actions/Integrating%20with%20Azure%20Pipelines/#canary","title":"Canary","text":"<p>Using this strategy, you can first roll out the changes to a small subset of servers. The canary deployment strategy is an advanced deployment strategy that helps mitigate the risk of rolling out new versions of applications.</p> <pre><code>strategy:\n    canary:\n        increments: [ number ]\n        preDeploy:       \n            pool: [ server | pool ] # See pool schema.       \n            steps:\n            - script: [ script | bash | pwsh | powershell | checkout | task | templateReference ]\n        deploy:         \n            pool: [ server | pool ] # See pool schema.       \n            steps:\n            ...\n        routeTraffic:       \n            pool: [ server | pool ]       \n            steps:\n            ...       \n        postRouteTraffic:         \n            pool: [ server | pool ]       \n            steps:\n            ...\n        on:\n            failure:       \n                pool: [ server | pool ]         \n                steps:\n                ...\n            success:         \n                pool: [ server | pool ]         \n                steps:\n                ...\n</code></pre>"},{"location":"Implement-CI-with-Azure-Pipelines-and-GitHub-Actions/Integrating%20with%20Azure%20Pipelines/#lifecycle-hooks","title":"Lifecycle hooks","text":"<p>You can achieve the deployment strategies technique by using lifecycle hooks. It's equivalent of <code>before_script</code> and <code>script</code> in GitLab CI/CD.</p> <p>Available lifecycle hooks: - <code>preDeploy</code>: Used to run steps that initialize resources before application deployment starts. - <code>deploy</code>: Used to run steps that deploy your application. Download artifact task will be auto-injected only in the deploy hook for deployment jobs. - <code>routeTraffic</code>: Used to run steps that serve the traffic to the updated version. - <code>postRouteTraffic</code>: Used to run the steps after the traffic is routed. Typically, these tasks monitor the health of the updated version for a defined interval.</p>"},{"location":"Implement-CI-with-Azure-Pipelines-and-GitHub-Actions/Integrating%20with%20Azure%20Pipelines/#template-references","title":"Template references","text":"<p>You can export reusable sections of your pipeline to a separate file. These individual files are known as templates.</p> <p>Azure Pipelines supports four types of templates:</p> <ul> <li>Stage.</li> <li>Job.</li> <li>Step.</li> <li>Variable.</li> </ul> <p>You can also use templates to control what is allowed in a pipeline and define how parameters can be used.</p>"},{"location":"Implement-CI-with-Azure-Pipelines-and-GitHub-Actions/Integrating%20with%20Azure%20Pipelines/#using-multiple-repositories-in-pipeline","title":"Using multiple repositories in pipeline","text":"<p>Repositories can be specified as a repository resource or in line with the checkout step. Supported repositories are Azure Repos Git, GitHub, and BitBucket Cloud.</p> <p>The following combinations of checkout steps are supported:</p> <ul> <li>If there are no <code>checkout</code> steps, the default behavior is <code>checkout: self</code> is the first step.</li> <li>If there's a single <code>checkout: none</code> step, no repositories are synced or checked out.</li> <li>If there's a single c<code>heckout: self</code> step, the current repository is checked out.</li> <li>If there's a single checkout step that isn't self or none, that repository is checked out instead of self.</li> <li>If there are multiple checkout steps, each named repository is checked out to a folder named after the repository. Unless a different path is specified in the checkout step, use <code>checkout: self</code> as one of the checkout steps.</li> </ul>"},{"location":"Implement-CI-with-Azure-Pipelines-and-GitHub-Actions/Integrating%20with%20Azure%20Pipelines/#github-repository","title":"GitHub repository","text":"<p>There are three authentication types for granting Azure Pipelines access to your GitHub repositories while creating a pipeline.</p> <ul> <li>GitHub App.</li> <li>OAuth.</li> <li>Personal access token (PAT).</li> </ul>"},{"location":"Implement-CI-with-Azure-Pipelines-and-GitHub-Actions/Managing%20Azure%20Pipeline%20agents%20and%20pools/","title":"Managing Azure Pipeline agents and pools","text":""},{"location":"Implement-CI-with-Azure-Pipelines-and-GitHub-Actions/Managing%20Azure%20Pipeline%20agents%20and%20pools/#microsoft-vs-self-hosted-agents","title":"Microsoft vs. self-hosted agents","text":"Microsoft agent Self-hosted agent Automatic maintanance and upgrades. You manage maintatnace and updates. Has job time limits. Doesn't have job time limits. Each time pipeline is run, new VM is spawned and discarded after one use. You can install agent on Linux, macOS, Windows or Docker containers."},{"location":"Implement-CI-with-Azure-Pipelines-and-GitHub-Actions/Managing%20Azure%20Pipeline%20agents%20and%20pools/#azure-devops-job-types","title":"Azure DevOps job types","text":"<ol> <li>Agent pool jobs: Most common type of jobs. The jobs run on an agent that is part of an agent pool.</li> <li>Container jobs: Similar jobs to Agent Pool Jobs run in a container on an agent part of an agent pool.</li> <li>Deployment group jobs: Jobs that run on systems in a deployment group.</li> <li>Agentless jobs: Jobs that run directly on the Azure DevOps. They don't require an agent for execution. It's also-often-called Server Jobs.</li> </ol>"},{"location":"Implement-CI-with-Azure-Pipelines-and-GitHub-Actions/Managing%20Azure%20Pipeline%20agents%20and%20pools/#agent-pools","title":"Agent pools","text":"<p>Instead of managing each agent individually, you organize agents into agent pools. An agent pool defines the sharing boundary for all agents in that pool.</p> <p>In Azure Pipelines, pools are scoped to the entire organization so that you can share the agent machines across projects. If you create an Agent pool for a specific project, only that project can use the pool until you add the project pool into another project.</p> <p>When creating a build or release pipeline, you can specify which pool it uses, organization, or project scope. Pools scoped to a project can only use them across build and release pipelines within a project. To share an agent pool with multiple projects, use an organization scope agent pool and add them in each of those projects, add an existing agent pool, and choose the organization agent pool. </p>"},{"location":"Implement-CI-with-Azure-Pipelines-and-GitHub-Actions/Managing%20Azure%20Pipeline%20agents%20and%20pools/#predefined-agent-pools","title":"Predefined agent pools","text":"<p>Azure Pipelines provides a pre-defined agent pool-named Azure Pipelines with Microsoft-hosted agents. It will often be an easy way to run jobs without configuring build infrastructure. The following virtual machine images are provided by default:</p> <ul> <li>Windows Server 2022 with Visual Studio 2022.</li> <li>Windows Server 2019 with Visual Studio 2019.  </li> <li>Ubuntu 22.04.     </li> <li>Ubuntu 20.04.     </li> <li>macOS 13 Ventura.     </li> <li>macOS 12 Monterey.    </li> <li>macOS 11 Big Sur. </li> </ul> <p>By default, all contributors in a project are members of the User role on each hosted pool. It allows every contributor to the author and runs build and release pipelines using a Microsoft-hosted pool.</p>"},{"location":"Implement-CI-with-Azure-Pipelines-and-GitHub-Actions/Managing%20Azure%20Pipeline%20agents%20and%20pools/#communication-between-agent-and-azure-pipelines","title":"Communication between agent and Azure Pipelines","text":"<p>Agent always starts communication. Traffic betwwen agent and Azure Pipelines is encrypted using asymmetric encryption.</p> <ol> <li>User registers an agent with Azure Pipelines by adding it to an agent pool (you must have admin privileges to do that).</li> <li>After registration is complete, agent downloads an OAuth token and uses it to listen to the job queue.</li> <li>Periodically, the agent checks to see if a new job request has been posted in the job queue in Azure Pipelines, if so the agent downloads the job and job-specific OAuth token when available (used to access resources needed to complete the job).</li> <li>Once the job is completed, the agent discards the job-specific OAuth token and checks if there's a new job request using the listener OAuth token.</li> </ol>"},{"location":"Implement-CI-with-Azure-Pipelines-and-GitHub-Actions/Managing%20Azure%20Pipeline%20agents%20and%20pools/#communication-with-job-target-servers","title":"Communication with job target servers","text":"<p>To deploy artifacts to the server, you need \"line-of-sight\" access to them. By default all Microsoft-hosted agent pools have connectivity to Azure websites and servers running in Azure.</p> <p>In case of on-premise environments, you will need to configure self-hosted agent to deploy.</p> <p></p>"},{"location":"Implement-CI-with-Azure-Pipelines-and-GitHub-Actions/Managing%20Azure%20Pipeline%20agents%20and%20pools/#interactive-vs-service","title":"Interactive vs. service","text":"<p>You may run your agent as a service or an interactive process. For example, to run tasks that use Windows authentication to access an external service, you must run the agent using an account with access to that service. However, if you're running UI tests such as Selenium or Coded UI tests that require a browser, the browser is launched in the context of the agent account.</p>"},{"location":"Implement-CI-with-Azure-Pipelines-and-GitHub-Actions/Managing%20Azure%20Pipeline%20agents%20and%20pools/#do-self-hosted-agents-have-any-performance-advantages-over-microsoft-hosted-agents","title":"Do self-hosted agents have any performance advantages over Microsoft-hosted agents?","text":"<p>In many cases, yes. Specifically:</p> <ul> <li>If you use a self-hosted agent, you can run incremental builds. For example, you define a CI build pipeline that doesn't clean the repo or do a clean build. Your builds will typically run faster.<ul> <li>You don't get these benefits when using a Microsoft-hosted agent. The agent is destroyed after the build or release pipeline is completed.</li> </ul> </li> <li>A Microsoft-hosted agent can take longer to start your build. While it often takes just a few seconds for your job to be assigned to a Microsoft-hosted agent, it can sometimes take several minutes for an agent to be allocated, depending on the load on our system.</li> </ul>"},{"location":"Implement-CI-with-Azure-Pipelines-and-GitHub-Actions/Managing%20Azure%20Pipeline%20agents%20and%20pools/#can-i-install-multiple-self-hosted-agents-on-the-same-machine","title":"Can I install multiple self-hosted agents on the same machine?","text":"<p>Yes. This approach can work well for agents who run jobs that don't consume many shared resources. For example, you could try it for agents that run releases that mostly orchestrate deployments and don't do much work on the agent itself. In other cases, you might find that you don't gain much efficiency by running multiple agents on the same machine.</p>"},{"location":"Implement-CI-with-Azure-Pipelines-and-GitHub-Actions/Managing%20Azure%20Pipeline%20agents%20and%20pools/#security-of-agent-pools","title":"Security of agent pools","text":"<p>In Azure Pipelines, roles are defined on each agent pool.</p> Role on an organization agent pool Purpose Reader View the organization's agent pool and agents. Service Account Use the organization agent pool to create a project agent pool in a project. Administrator Register or unregister agents from the organization's agent pool. They can also refer to the organization agent pool when creating a project agent pool in a project. Finally, they can also manage membership for all roles of the organization agent pool. Role on an project agent pool Purpose Reader View the project agent pool and agents. Service Account Use the project agent pool when authoring build or release pipelines. Administrator Manage membership for all roles of the project agent pool. The user that created the pool is automatically added to the Administrator role for that pool."},{"location":"Implement-CI-with-Azure-Pipelines-and-GitHub-Actions/Managing%20Azure%20Pipeline%20agents%20and%20pools/#lab-address","title":"Lab address","text":"<p>https://aka.ms/az-400-configure-agent-pools-and-understand-pipeline-styles</p>"},{"location":"Implement-CI-with-Azure-Pipelines-and-GitHub-Actions/Pipeline%20strategy/","title":"Pipeline strategy","text":""},{"location":"Implement-CI-with-Azure-Pipelines-and-GitHub-Actions/Pipeline%20strategy/#agent-demands","title":"Agent demands","text":"<p>Each agent has a config stored as key-value pairs. System capabilities are automatically discovered (things like machone name or system type). The ones configured by user are User-defined capabilities.</p>"},{"location":"Implement-CI-with-Azure-Pipelines-and-GitHub-Actions/Pipeline%20strategy/#implementing-multi-agent-builds","title":"Implementing multi-agent builds","text":"<p>You can use multiple build agents to support multiple build machines. Either distribute the load, run builds in parallel, or use different agent capabilities.</p> <p>Adding multiple jobs to a pipeline lets you:</p> <ul> <li>Break your pipeline into sections that need different agent pools or self-hosted agents.</li> <li>Publish artifacts in one job and consume them in one or more subsequent jobs.</li> <li>Build faster by running multiple jobs in parallel.</li> <li>Enable conditional execution of tasks.</li> </ul> <p>At the organization level, you can configure the number of parallel jobs that are made available.</p>"},{"location":"Implement-a-secure-continuous-deployment-using-Azure-Pipelines/AB%20testing%20and%20progressive%20exposure%20deployment/","title":"A/B testing and progressive exposure deployment","text":""},{"location":"Implement-a-secure-continuous-deployment-using-Azure-Pipelines/AB%20testing%20and%20progressive%20exposure%20deployment/#ab-testing","title":"A/B testing","text":"<p>A/B testing (also known as split testing or bucket testing) compares two versions of a web page or app against each other to determine which one does better. A/B testing is mainly an experiment where two or more page variants are shown to users at random. Also, statistical analysis is used to determine which variation works better for a given conversion goal.</p>"},{"location":"Implement-a-secure-continuous-deployment-using-Azure-Pipelines/AB%20testing%20and%20progressive%20exposure%20deployment/#ci-cd-with-deployment-rings","title":"CI-CD with deployment rings","text":"<p>Progressive exposure deployment, also called ring-based deployment, was first discussed in Jez Humble's Continuous Delivery book. They support the production-first DevOps mindset and limit the impact on end users while gradually deploying and validating changes in production.</p> <p>Impact (also called blast radius) is evaluated through observation, testing, analysis of telemetry, and user feedback. In DevOps, rings are typically modeled as stages. Rings are, in essence, an extension of the canary stage. The canary release releases to a stage to measure impact. Adding another ring is essentially the same thing.</p> <p>With a ring-based deployment, you first deploy your changes to risk-tolerant customers and progressively roll out to a more extensive set of customers.</p> <p></p>"},{"location":"Implement-a-secure-continuous-deployment-using-Azure-Pipelines/Blue-green%20deployment%20and%20feature%20toggles/","title":"Blue-green deployment and feature toggles","text":""},{"location":"Implement-a-secure-continuous-deployment-using-Azure-Pipelines/Blue-green%20deployment%20and%20feature%20toggles/#blue-green-deployment","title":"Blue-green deployment","text":"<p>Blue-green deployment is a technique that reduces risk and downtime by running two identical environments. These environments are called blue and green. Only one of the environments is live, with the live environment serving all production traffic.</p> <p></p> <p>For this example, blue is currently live, and green is idle.</p> <p>As you prepare a new version of your software, the deployment and final testing stage occur in an environment that isn't live: in this example, green. Once you've deployed and thoroughly tested the software in green, switch the router or load balancer so all incoming requests go to green instead of blue.</p> <p>Green is now live, and blue is idle.</p>"},{"location":"Implement-a-secure-continuous-deployment-using-Azure-Pipelines/Blue-green%20deployment%20and%20feature%20toggles/#deployment-slots","title":"Deployment slots","text":"<p>Deployment slots are a feature of Azure App Service. They're live apps with their hostnames. You can create different slots for your application (for example, Dev, Test, or Stage). The production slot is the slot where your live app stays. You can validate app changes in staging with deployment slots before swapping them with your production slot.</p>"},{"location":"Implement-a-secure-continuous-deployment-using-Azure-Pipelines/Blue-green%20deployment%20and%20feature%20toggles/#swap","title":"Swap","text":"<p>The swap eliminates downtime when you deploy your app with seamless traffic redirection, and no requests are dropped because of swap operations.</p>"},{"location":"Implement-a-secure-continuous-deployment-using-Azure-Pipelines/Blue-green%20deployment%20and%20feature%20toggles/#feature-flags","title":"Feature flags","text":"<p>Feature Flags allow you to change how our system works without making significant changes to the code. Only a small configuration change is required. In many cases, it will also only be for a few users. Feature Flags offer a solution to the need to push new code into the trunk and deploy it, but it isn't functional yet.</p> <p>They're commonly implemented as the value of variables used to control conditional logic.</p>"},{"location":"Implement-a-secure-continuous-deployment-using-Azure-Pipelines/Blue-green%20deployment%20and%20feature%20toggles/#feature-toggles","title":"Feature toggles","text":"<p>Feature toggles are also known as feature flippers, feature flags, feature switches, conditional features, and so on. Besides the power they give you on the business side, they also provide an advantage on the development side. Feature toggles are a great alternative to branching as well. Branching is what we do in our version control system.</p> <p></p> <p>When the switch is off, it executes the code in the IF, otherwise the ELSE. You can make it much more intelligent, controlling the feature toggles from a dashboard or building capabilities for roles, users, and so on.</p>"},{"location":"Implement-a-secure-continuous-deployment-using-Azure-Pipelines/Canary%20release%20and%20dark%20launching/","title":"Canary release and dark launching","text":""},{"location":"Implement-a-secure-continuous-deployment-using-Azure-Pipelines/Canary%20release%20and%20dark%20launching/#canary-releases","title":"Canary releases","text":"<p>A canary release is a way to identify potential problems without exposing all your end users to the issue at once. The idea is that you tell a new feature only to a minimal subset of users. By closely monitoring what happens when you enable the feature, you can get relevant information from this set of users and either continue or rollback (disable the feature).</p> <p>If the canary release shows potential performance or scalability problems, you can build a fix for that and apply that in the canary environment. After the canary release has proven to be stable, you can move the canary release to the actual production environment.</p>"},{"location":"Implement-a-secure-continuous-deployment-using-Azure-Pipelines/Canary%20release%20and%20dark%20launching/#azure-traffic-manager","title":"Azure Traffic Manager","text":"<p>Azure Traffic Manager is a DNS-based traffic load balancer that enables you to distribute traffic optimally to services across global Azure regions while providing high availability and responsiveness. Traffic Manager uses DNS to direct client requests to the most appropriate service endpoint based on a traffic-routing method and the health of the endpoints.</p> <p>While the available options can change over time, the Traffic Manager currently provides six options to distribute traffic:</p> <ul> <li>Priority: Select Priority when you want to use a primary service endpoint for all traffic and provide backups if the primary or the backup endpoints are unavailable.</li> <li>Weighted: Select Weighted when you want to distribute traffic across a set of endpoints, either evenly or according to weights, which you define.</li> <li>Performance: Select Performance when you have endpoints in different geographic locations, and you want end users to use the \"closest\" endpoint for the lowest network latency.</li> <li>Geographic: Select Geographic so that users are directed to specific endpoints (Azure, External, or Nested) based on which geographic location their DNS query originates from. It empowers Traffic Manager customers to enable scenarios where knowing a user's geographic region and routing them based on that is necessary. Examples include following data sovereignty mandates, localization of content &amp; user experience, and measuring traffic from different regions.</li> <li>Multivalue: Select MultiValue for Traffic Manager profiles that can only have IPv4/IPv6 addresses as endpoints. When a query is received for this profile, all healthy endpoints are returned.</li> <li>Subnet: Select the Subnet traffic-routing method to map sets of end-user IP address ranges to a specific endpoint within a Traffic Manager profile. The endpoint returned will be mapped for that request's source IP address when a request is received.</li> </ul>"},{"location":"Implement-a-secure-continuous-deployment-using-Azure-Pipelines/Canary%20release%20and%20dark%20launching/#dark-launching","title":"Dark launching","text":"<p>Dark launching is in many ways like canary releases. However, the difference here's that you're looking to assess users' responses to new features in your frontend rather than testing the performance of the backend.</p> <p>The idea is that rather than launch a new feature for all users, you instead release it to a small set of users. Usually, these users aren't aware they're being used as test users for the new feature, and often you don't even highlight the new feature to them, as such the term \"Dark\" launching. Another example of dark launching is launching a new feature and using it on the backend to get metrics.</p>"},{"location":"Implement-a-secure-continuous-deployment-using-Azure-Pipelines/Integrating%20identity%20management%20systems/","title":"Integrating identity management systems","text":""},{"location":"Implement-a-secure-continuous-deployment-using-Azure-Pipelines/Integrating%20identity%20management%20systems/#github-sso","title":"GitHub SSO","text":"<p>Offers both SAML and SCIM support. You need to connect your identity provider at the organization level to use GitHub SSO.</p>"},{"location":"Implement-a-secure-continuous-deployment-using-Azure-Pipelines/Integrating%20identity%20management%20systems/#service-principals","title":"Service Principals","text":"<p>Azure AD offers different kinds of mechanisms for authentication. In DevOps Projects, though, one of the most important is the use of Service Principals.</p>"},{"location":"Implement-a-secure-continuous-deployment-using-Azure-Pipelines/Integrating%20identity%20management%20systems/#azure-ad-applications","title":"Azure AD applications","text":"<p>Applications are registered with an Azure AD tenant within Azure Active Directory. Registering an application creates an identity configuration. You also determine who can use it:</p> <ul> <li>Accounts in the same organizational directory.</li> <li>Accounts in any organizational directory.</li> <li>Accounts in any organizational directory and Microsoft Accounts (personal).</li> <li>Microsoft Accounts (Personal accounts only).</li> </ul> <p>Once the application is created, you then should create at least one client secret for the application.</p> <p>The application identity can then be granted permissions within services and resources that trust Azure Active Directory.</p> <p>To access resources, an entity must be represented by a security principal. To connect, the entity must know:</p> <ul> <li>TenantID.</li> <li>ApplicationID.</li> <li>Client Secret.</li> </ul>"},{"location":"Implement-a-secure-continuous-deployment-using-Azure-Pipelines/Integrating%20identity%20management%20systems/#managed-identity","title":"Managed identity","text":"<p>There are two types of managed identities:</p> <ul> <li>System-assigned: It's the types of identities which are typical for most of Azure resources. Many, but not all, services expose these identities.</li> <li>User-assigned: You can create a managed identity as an Azure resource. It can then be assigned to one or more instances of a service.</li> </ul>"},{"location":"Implement-a-secure-continuous-deployment-using-Azure-Pipelines/Introdcution%20to%20deployment%20patterns/","title":"Introdcution to deployment patterns","text":""},{"location":"Implement-a-secure-continuous-deployment-using-Azure-Pipelines/Introdcution%20to%20deployment%20patterns/#microservices-architecture","title":"Microservices architecture","text":"<p>A microservice is an autonomous, independently deployable, and scalable software component. They're small, focused on doing one thing well, and can run autonomously. If one microservice changes, it shouldn't impact any other microservices within your landscape.</p> <p>Each microservice has its lifecycle and Continuous Delivery pipeline. If you built them correctly, you could deploy new microservice versions without impacting other system parts. Microservice architecture is undoubtedly not a prerequisite for Continuous Delivery, but smaller software components help implement a fully automated pipeline.</p>"},{"location":"Implement-a-secure-continuous-deployment-using-Azure-Pipelines/Introdcution%20to%20deployment%20patterns/#classical-deployment-patterns","title":"Classical deployment patterns","text":""},{"location":"Implement-a-secure-continuous-deployment-using-Azure-Pipelines/Introdcution%20to%20deployment%20patterns/#modern-deployment-patterns","title":"Modern deployment patterns","text":"<ul> <li>Blue-green deployments.</li> <li>Canary releases.</li> <li>Dark launching.</li> <li>A/B testing.</li> <li>Progressive exposure or ring-based deployment.</li> <li>Feature toggles.</li> </ul>"},{"location":"Implement-a-secure-continuous-deployment-using-Azure-Pipelines/Managing%20application%20configuration%20data/","title":"Managing application configuration data","text":""},{"location":"Implement-a-secure-continuous-deployment-using-Azure-Pipelines/Managing%20application%20configuration%20data/#external-configuration-store-patterns","title":"External configuration store patterns","text":"<p>These patterns store the configuration information in an external location and provide an interface that can be used to quickly and efficiently read and update configuration settings.</p> <p>The type of external store depends on the hosting and runtime environment of the application. A cloud-hosted scenario is typically a cloud-based storage service but could be a hosted database or other systems. The backing store you choose for configuration information should have an interface that provides consistent and easy-to-use access.</p> <p>It should expose the information in a correctly typed and structured format.</p> <p></p>"},{"location":"Implement-a-secure-continuous-deployment-using-Azure-Pipelines/Managing%20application%20configuration%20data/#azure-app-configuration","title":"Azure App Configuration","text":"<p>Azure App Configuration is a service for central management of application settings and feature flags. Azure App Configuration service stores all the settings for your application and secures their access in one place.</p> <p>Azure App Configuration service provides the following features: - A fully managed service that can be set up in minutes. - Flexible key representations and mappings. - Tagging with labels. - A point-in-time replay of settings. - Dedicated UI for feature flag management. - Comparison of two sets of configurations on custom-defined dimensions. - Enhanced security through Azure managed identities. - Complete data encryptions, at rest or in transit. - Native integration with popular frameworks.</p> <p>App Configuration complements Azure Key Vault, which is used to store application secrets. App Configuration makes it easier to implement the following scenarios:</p> <ul> <li>Centralize management and distribution of hierarchical configuration data for different environments and geographies.</li> <li>Dynamically change application settings without the need to redeploy or restart an application. Control feature availability in real time.</li> </ul> Programming language and framework How to connect .NET Core and ASP.NET Core App Configuration provider for .NET Core .NET Framework and ASP.NET App Configuration builder for .NET Java Spring App Configuration client for Spring Cloud Others App Configuration REST API"},{"location":"Implement-a-secure-continuous-deployment-using-Azure-Pipelines/Managing%20application%20configuration%20data/#key-value-pairs","title":"Key-value pairs","text":"<p>Keys serve as the name for key-value pairs and are used to store and retrieve corresponding values. </p> <ul> <li>App Configuration treats keys as a whole. It doesn't parse keys to figure out how their names are structured or enforce any rule on them.</li> <li>Keys stored in App Configuration are case-sensitive, Unicode-based strings.</li> <li>You can use any Unicode character in key names entered into App Configuration except for <code>*</code>, <code>,</code>, and <code>\\</code>. These characters are reserved. If you need to include a reserved character, you must escape it by using<code>\\{Reserved Character}</code>.</li> <li>There's a combined size limit of 10,000 characters on a key-value pair. This limit includes all characters in the key, its value, and all associated optional attributes.</li> </ul>"},{"location":"Implement-a-secure-continuous-deployment-using-Azure-Pipelines/Managing%20application%20configuration%20data/#key-namespaces","title":"Key namespaces","text":"<p>Below are some examples of how you can structure your key names into a hierarchy:</p> <pre><code>AppName:Service1:ApiEndpoint\nAppName:Service2:ApiEndpoint\n</code></pre> <pre><code>AppName:Region1:DbEndpoint\nAppName:Region2:DbEndpoint\n</code></pre>"},{"location":"Implement-a-secure-continuous-deployment-using-Azure-Pipelines/Managing%20application%20configuration%20data/#label-keys","title":"Label keys","text":"<p>Key values in App Configuration can optionally have a label attribute. Labels are used to differentiate key values with the same key. A key app1 with labels A and B forms two separate keys in an App Configuration store. By default, the label for a key value is empty or null. Label provides a convenient way to create variants of a key.</p> <p>A common use of labels is to specify multiple environments for the same key:</p> <pre><code>Key = AppName:DbEndpoint &amp; Label = Test\nKey = AppName:DbEndpoint &amp; Label = Staging\nKey = AppName:DbEndpoint &amp; Label = Production\n</code></pre>"},{"location":"Implement-a-secure-continuous-deployment-using-Azure-Pipelines/Managing%20application%20configuration%20data/#query-key-values","title":"Query key values","text":"<p>Each key value is uniquely identified by its key plus a label that can be null. You query an App Configuration store for key values by specifying a pattern. The App Configuration store returns all key values that match the pattern and their corresponding values and attributes.</p>"},{"location":"Implement-a-secure-continuous-deployment-using-Azure-Pipelines/Managing%20application%20configuration%20data/#app-configuration-feature-management","title":"App configuration feature management","text":"<p>Azure App Configuration Service can be used to store and manage feature flags. (It's also known as feature toggles, feature switches, and other names).</p>"},{"location":"Implement-a-secure-continuous-deployment-using-Azure-Pipelines/Managing%20application%20configuration%20data/#basic-concepts","title":"Basic concepts","text":"<ul> <li>Feature flag: A feature flag is a variable with a binary state of on or off. The feature flag also has an associated code block. The state of the feature flag triggers whether the code block runs or not.</li> <li>Feature manager: A feature manager is an application package that handles the lifecycle of all the feature flags in an application. The feature manager typically provides more functionality, such as caching feature flags and updating their states.</li> <li>Filter: A filter is a rule for evaluating the state of a feature flag. A user group, a device or browser type, a geographic location, and a time window are all examples of what a filter can represent.</li> </ul>"},{"location":"Implement-a-secure-continuous-deployment-using-Azure-Pipelines/Managing%20application%20configuration%20data/#basic-feature-flag-usage-in-code","title":"Basic feature flag usage in code","text":"<p>The basic pattern for implementing feature flags in an application is simple. You can think of a feature flag as a Boolean state variable used with an <code>if</code> conditional statement in your code:</p> <pre><code>if (featureFlag) {\n    // Run the following code.\n}\n</code></pre>"},{"location":"Implement-a-secure-continuous-deployment-using-Azure-Pipelines/Managing%20application%20configuration%20data/#feature-flag-declaration","title":"Feature flag declaration","text":"<p>Each feature flag has two parts: a name and a list of one or more filters used to evaluate if a feature's state is on (that is when its value is True).</p> <p>A filter defines a use case for when a feature should be turned on. When a feature flag has multiple filters, the filter list is traversed until one of the filters determines the feature should be enabled.</p> <p>At that point, the feature flag is on, and any remaining filter results are skipped. If no filter indicates the feature should be enabled, the feature flag is off. The feature manager supports <code>appsettings.json</code> as a configuration source for feature flags. The following example shows how to set up feature flags in a JSON file:</p> <pre><code>\"FeatureManagement\": {\n    \"FeatureA\": true, // Feature flag set to on\n    \"FeatureB\": false, // Feature flag set to off\n    \"FeatureC\": {\n        \"EnabledFor\": [\n            {\n                \"Name\": \"Percentage\",\n                \"Parameters\": {\n                    \"Value\": 50\n                }\n            }\n        ]\n    }\n}\n</code></pre>"},{"location":"Implement-a-secure-continuous-deployment-using-Azure-Pipelines/Managing%20application%20configuration%20data/#managing-secrets-tokens-and-certificates","title":"Managing secrets, tokens and certificates","text":"<p>Azure Key Vault helps solve the following problems:</p> <ul> <li>Secrets management: Azure Key Vault can be used to store securely and tightly control access to tokens, passwords, certificates, API keys, and other secrets.</li> <li>Key management: Azure Key Vault can also be used as a key management solution. Azure Key Vault makes it easy to create and control the encryption keys used to encrypt your data.</li> <li>Certificate management: Azure Key Vault is also a service that lets you easily provision, manage, and deploy public and private Secure Sockets Layer/Transport Layer Security (SSL/TLS) certificates for use with Azure. And your internal connected resources.</li> <li>Store secrets backed by hardware security modules: The secrets and keys can be protected by software or FIPS 140-2 Level 2 validates HSMs.</li> </ul>"},{"location":"Implement-a-secure-continuous-deployment-using-Azure-Pipelines/Managing%20application%20configuration%20data/#using-azure-key-value","title":"Using Azure Key Value","text":""},{"location":"Implement-a-secure-continuous-deployment-using-Azure-Pipelines/Managing%20application%20configuration%20data/#centralize-application-secrets","title":"Centralize application secrets","text":"<p>Centralizing the storage of application secrets in Azure Key Vault allows you to control their distribution. Key Vault dramatically reduces the chances that secrets may be accidentally leaked.</p>"},{"location":"Implement-a-secure-continuous-deployment-using-Azure-Pipelines/Managing%20application%20configuration%20data/#securely-store-secrets-and-keys","title":"Securely store secrets and keys","text":"<p>Secrets and keys are safeguarded by Azure, using industry-standard algorithms, key lengths, and hardware security modules (HSMs). The HSMs used are Federal Information Processing Standards (FIPS) 140-2 Level 2 validated.</p>"},{"location":"Implement-a-secure-continuous-deployment-using-Azure-Pipelines/Managing%20application%20configuration%20data/#monitor-access-and-use","title":"Monitor access and use","text":"<p>Once you've created a couple of Key Vaults, you'll want to monitor how and when your keys and secrets are accessed. You can do it by enabling logging for Key Vault. You can configure Azure Key Vault to:</p> <ul> <li>Archive to a storage account.</li> <li>Stream to an Event Hubs.</li> <li>Send the logs to Log Analytics.</li> </ul>"},{"location":"Implement-a-secure-continuous-deployment-using-Azure-Pipelines/Managing%20application%20configuration%20data/#simplified-administration-of-application-secrets","title":"Simplified administration of application secrets","text":"<p>Azure Key Vault simplifies it by:</p> <ul> <li>Removing the need for in-house knowledge of Hardware Security Modules.</li> <li>Scaling up on short notice to meet your organization's usage spikes.</li> <li>Replicating the contents of your Key Vault within a region and to a secondary region. It ensures high availability and takes away the need for any action from the administrator to trigger the failover.</li> <li>Providing standard Azure administration options via the portal, Azure CLI and PowerShell.</li> <li>Automating specific tasks on certificates that you purchase from Public CAs, such as enrollment and renewal.</li> </ul>"},{"location":"Implement-a-secure-continuous-deployment-using-Azure-Pipelines/Managing%20application%20configuration%20data/#integrate-with-other-azure-services","title":"Integrate with other Azure services","text":"<p>As a secure store in Azure, Key Vault has been used to simplify scenarios like Azure Disk Encryption, the always encrypted functionality in SQL Server and Azure SQL Database, Azure web apps. Key Vault itself can integrate with storage accounts, Event Hubs, and log analytics.</p>"},{"location":"Implement-a-secure-continuous-deployment-using-Azure-Pipelines/Managing%20application%20configuration%20data/#devops-inner-and-outer-loop","title":"DevOps inner and outer loop","text":"<p>The Inner loop is focused on the developer teams iterating over their solution development; they consume the configuration published by the Outer Loop.</p> <p>The Ops Engineer governs the Configuration management. They push changes into Azure KeyVault and Kubernetes that are further isolated per environment.</p> <p></p>"},{"location":"Implement-a-secure-continuous-deployment-using-Azure-Pipelines/Managing%20application%20configuration%20data/#lab-address","title":"Lab address","text":"<p>Integrate Azure Key Vault with Azure DevOps: https://aka.ms/az-400-integrate-azure-key-vault-with-azure-devops</p> <p>Enable Dynamic Configuration and Feature Flags: https://aka.ms/az-400-enable-dynamic-configuration-and-feature-flags</p>"},{"location":"Implement-continuous-feedback/Developing%20monitor%20and%20status%20dashboards/","title":"Developing monitor and status dashboards","text":""},{"location":"Implement-continuous-feedback/Developing%20monitor%20and%20status%20dashboards/#azure-dashboards","title":"Azure Dashboards","text":"<p>Azure dashboards are the primary dashboarding technology for Azure. However, you can access data in log and metric data in Azure Monitor through their API using any REST client.</p>"},{"location":"Implement-continuous-feedback/Developing%20monitor%20and%20status%20dashboards/#limitations-of-azure-dashboards","title":"Limitations of Azure Dashboards","text":"<ul> <li>Limited control over log visualizations with no support for data tables. The total number of data series is limited to 10, with different data series grouped under another bucket.</li> <li>No custom parameters support for log charts.</li> <li>Log charts are limited to the last 30 days.</li> <li>Log charts can only be pinned to shared dashboards.</li> <li>No interactivity with dashboard data.</li> <li>Limited contextual drill-down.</li> </ul>"},{"location":"Implement-continuous-feedback/Developing%20monitor%20and%20status%20dashboards/#view-designer-in-azure-monitor","title":"View designer in Azure Monitor","text":"<p>View Designer in Azure Monitor allows you to create custom visualizations with log data. They're used by monitoring solutions to present the data they collect.</p>"},{"location":"Implement-continuous-feedback/Developing%20monitor%20and%20status%20dashboards/#limitations-of-view-designer-in-azure-monitor","title":"Limitations of view designer in Azure Monitor","text":"<ul> <li>Supports logs but not metrics.</li> <li>No personal views. Available to all users with access to the workspace.</li> <li>No automatic refresh.</li> <li>Limited layout options.</li> <li>No support for querying across multiple workspaces or Application Insights applications.</li> <li>Queries are limited in response size to 8 MB and query execution time of 110 seconds.</li> </ul>"},{"location":"Implement-continuous-feedback/Developing%20monitor%20and%20status%20dashboards/#azure-monitor-workbooks","title":"Azure Monitor workbooks","text":"<p>Workbooks are interactive documents that provide deep insights into your data, investigation, and collaboration inside the team. Specific examples where workbooks are helpful are troubleshooting guides and incident postmortem.</p>"},{"location":"Implement-continuous-feedback/Developing%20monitor%20and%20status%20dashboards/#limitations-of-azure-monitor-workbooks","title":"Limitations of Azure Monitor workbooks","text":"<ul> <li>No automatic refresh.</li> <li>No dense layout like dashboards, which make workbooks less useful as a single pane of glass. It's intended more for providing more profound insights.</li> </ul>"},{"location":"Implement-continuous-feedback/Developing%20monitor%20and%20status%20dashboards/#power-bi","title":"Power BI","text":"<p>Power BI is beneficial for creating business-centric dashboards and reports analyzing long-term KPI trends. You can import the results of a log query into a Power BI dataset to take advantage of its features, such as combining data from different sources and sharing reports on the web and mobile devices.</p>"},{"location":"Implement-continuous-feedback/Developing%20monitor%20and%20status%20dashboards/#limitations-of-power-bi","title":"Limitations of Power BI","text":"<ul> <li>It supports logs but not metrics.</li> <li>No Azure RM integration. Can't manage dashboards and models through Azure Resource Manager.</li> <li>Need to import query results need into the Power BI model to configure. Limitation on result size and refresh.</li> </ul>"},{"location":"Implement-continuous-feedback/Implementing%20tools%20to%20track%20usage%20and%20flow/","title":"Implementing tools to track usage and flow","text":""},{"location":"Implement-continuous-feedback/Implementing%20tools%20to%20track%20usage%20and%20flow/#continuous-monitoring","title":"Continuous monitoring","text":"<p>Continuous monitoring refers to the process and technology required to incorporate monitoring across each DevOps and IT operations lifecycles phase.</p> <p>Azure Monitor is the unified monitoring solution in Azure that provides full-stack observability across applications and infrastructure in the cloud and on-premises.</p>"},{"location":"Implement-continuous-feedback/Implementing%20tools%20to%20track%20usage%20and%20flow/#enabling-monitoring-for-all-applications","title":"Enabling monitoring for all applications","text":"<ul> <li>Azure DevOps Projects gives you a simplified experience with your existing code and Git repository or choose from one of the sample applications to create a Continuous Integration (CI) and Continuous Delivery (CD) pipeline to Azure.</li> <li>Continuous monitoring in your DevOps release pipeline allows you to gate or roll back your deployment based on monitoring data.</li> <li>Status Monitor allows you to instrument a live .NET app on Windows with Azure Application Insights without modifying or redeploying your code.</li> <li>If you have access to the code for your application, then enable complete monitoring with Application Insights by installing the Azure Monitor Application Insights SDK for .NET, Java, Node.js, or any other programming language. It lets you specify custom events, metrics, or page views relevant to your application and business.</li> </ul>"},{"location":"Implement-continuous-feedback/Implementing%20tools%20to%20track%20usage%20and%20flow/#enabling-monitoring-for-entire-infrastructure","title":"Enabling monitoring for entire infrastructure","text":"<ul> <li>You automatically get platform metrics, activity logs, and diagnostics logs from most of your Azure resources with no configuration.</li> <li>Enable deeper monitoring for VMs with Azure Monitor.</li> <li>Enable deeper monitoring for AKS clusters with Azure Monitor for containers.</li> <li>Add monitoring solutions for different applications and services in your environment.</li> <li>Use Resource Manager templates to enable monitoring and configure alerts over a large set of resources.</li> <li>Use Azure Policy to enforce different rules over your resources. It ensures those resources comply with your corporate standards and service level agreements.</li> </ul>"},{"location":"Implement-continuous-feedback/Implementing%20tools%20to%20track%20usage%20and%20flow/#ensuring-quality-through-continuous-deployment","title":"Ensuring quality through Continuous Deployment","text":"<ul> <li>Use Azure Pipelines to implement Continuous Deployment and automate your entire process from code commit to production based on your CI/CD tests.</li> <li>Use Quality Gates to integrate monitoring into your pre-deployment or post-deployment. It ensures that you meet the key health/performance metrics (KPIs) as your applications move from dev to production. Any differences in the infrastructure environment or scale aren't negatively impacting your KPIs.</li> <li>Maintain separate monitoring instances between your different deployment environments, such as Dev, Test, Canary, and Prod. It ensures that collected data is relevant across the associated applications and infrastructure. If you need to correlate data across environments, use multi-resource charts in Metrics Explorer or create cross-resource queries in Log Analytics.</li> </ul>"},{"location":"Implement-continuous-feedback/Implementing%20tools%20to%20track%20usage%20and%20flow/#azure-monitor-and-log-analytics","title":"Azure Monitor and Log Analytics","text":"<p>Azure Monitor is Microsoft's native cloud monitoring solution. Azure Monitor collects monitoring telemetry from different kinds of on-premises and Azure sources. Azure Monitor provides Management tools, such as those in Azure Security Center and Azure Automation, enabling ingestion of custom log data to Azure.</p> <p></p>"},{"location":"Implement-continuous-feedback/Implementing%20tools%20to%20track%20usage%20and%20flow/#how-it-works","title":"How it works","text":"<ol> <li>Log Analytics works by running the Microsoft Monitoring Agent service on the machine. The service locally captures and buffers the events and pushes them securely out to the Log Analytics workspace in Azure.</li> <li>Log into the virtual machine, navigate to the C:\\Program Files\\Microsoft Monitoring Agent\\MMA, and open the control panel. It will show you the details of the log analytics workspace connected. You also can add multiple log analytics workspaces to publish the log data into various workspaces.</li> </ol>"},{"location":"Implement-continuous-feedback/Implementing%20tools%20to%20track%20usage%20and%20flow/#kusto-query-language-kql","title":"Kusto Query Language (KQL)","text":"<p>Kusto is the primary way to query Log Analytics. It provides both a query language and a set of control commands. Kusto can be used directly within Azure Data Explorer. Azure Data Studio also offers a Kusto query experience and supports the creation of Jupiter-style notebooks for Kusto queries.</p>"},{"location":"Implement-continuous-feedback/Implementing%20tools%20to%20track%20usage%20and%20flow/#application-insights","title":"Application Insights","text":"<p>You install a small instrumentation package in your application and set up an Application Insights resource in the Microsoft Azure portal. The instrumentation monitors your app and sends telemetry data to the portal. (The application can run anywhere - it doesn't have to be hosted in Azure.) You can instrument the web service application, background components, and JavaScript in the web pages.</p> <p></p>"},{"location":"Implement-continuous-feedback/Implementing%20tools%20to%20track%20usage%20and%20flow/#what-do-aplication-insights-monitor","title":"What do Aplication Insights monitor?","text":"<ul> <li>Request rates, response times, and failure rates - Find out which pages are most popular, at what times of day, and where your users are. See which pages do best. If your response times and failure rates increase with more requests, perhaps you have a resourcing problem.</li> <li>Dependency rates, response times, and failure rates - Find out whether external services are slowing you down.</li> <li>Exceptions - Analyze the aggregated statistics, pick specific instances, and drill into the stack trace and related requests. Both server and browser exceptions are reported.</li> <li>Pageviews and load performance - reported by your users' browsers.</li> <li>AJAX calls from web pages - rates, response times, and failure rates.</li> <li>User and session count.</li> <li>Performance counters from your Windows or Linux server machines include CPU, memory, and network usage.</li> <li>Host diagnostics from Docker or Azure.</li> <li>Diagnostic trace logs from your app - so you can correlate trace events with requests.</li> <li>Custom events and metrics that you write yourself in the client or server code to track business events such as items sold or games won.</li> </ul>"},{"location":"Implement-continuous-feedback/Implementing%20tools%20to%20track%20usage%20and%20flow/#lab-address","title":"Lab address","text":"<p>https://aka.ms/az-400-monitor-application-performance-with-application-insights</p>"},{"location":"Implement-continuous-feedback/Managing%20alerts%2C%20blameless%20retrospectives%20and%20a%20just%20culture/","title":"Managing alerts","text":""},{"location":"Implement-continuous-feedback/Managing%20alerts%2C%20blameless%20retrospectives%20and%20a%20just%20culture/#examine-when-get-a-notification","title":"Examine when get a notification","text":"<p>Application Insights automatically analyzes the performance of your web application and can warn you about potential problems. You might be reading it because you received one of our smart detection notifications.</p> <p>Application Insights has detected that the performance of your application has degraded in one of these ways:</p> <ul> <li>Response time degradation: Your app has started responding to requests more slowly than it used to. The change might have been rapid, for example, because there was a regression in your latest deployment. Or it might have been gradual, maybe caused by a memory leak.</li> <li>Dependency duration degradation: Your app makes calls to a REST API, database, or other dependencies. The dependency is responding more slowly than it used to.</li> <li>Slow performance pattern: Your app appears to have a performance issue that is affecting only some requests. For example, pages are loading more slowly on one type of browser than others; or requests are being served more slowly from one server. Currently, our algorithms look at page load times, request response times, and dependency response times.</li> </ul>"},{"location":"Implement-continuous-feedback/Managing%20alerts%2C%20blameless%20retrospectives%20and%20a%20just%20culture/#improving-performance","title":"Improving performance","text":""},{"location":"Implement-continuous-feedback/Managing%20alerts%2C%20blameless%20retrospectives%20and%20a%20just%20culture/#triage","title":"Triage","text":"<p>First, does it matter? If a page is always slow to load, but only 1% of your site's users ever have to look at it, maybe you have more important things to think about. On the other hand, if only 1% of users open it, but it throws exceptions every time, that might be worth investigating. Use the impact statement (affected users or % of traffic) as a general guide but be aware that it isn't the whole story.</p>"},{"location":"Implement-continuous-feedback/Managing%20alerts%2C%20blameless%20retrospectives%20and%20a%20just%20culture/#diagnose-slow-page-loads","title":"Diagnose slow page loads","text":"<p>Where is the problem? Is the server slow to respond, is the page long, or does the browser have to do much work to display it? Open the Browsers metric blade. The segmented display of browser page load time shows where the time is going.</p> <ul> <li>If Send Request Time is high, the server responds slowly, or the request is a post with much data. Look at the performance metrics to investigate response times.</li> <li>Set up dependency tracking to see whether the slowness is because of external services or your database.</li> <li>If Receiving Response is predominant, your page and its dependent parts - JavaScript, CSS, images, and so on (but not asynchronously loaded data) are long. Set up an availability test and be sure to set the option to load dependent parts. When you get some results, open the detail of a result, and expand it to see the load times of different files.</li> <li>High Client Processing time suggests scripts are running slowly. If the reason isn't clear, consider adding some timing code and sending the times in track metrics calls.</li> </ul>"},{"location":"Implement-security-and-validate-code-bases-for-compliance/Introduction%20to%20Secure%20DevOps/","title":"Introduction to Secure DevOps","text":""},{"location":"Implement-security-and-validate-code-bases-for-compliance/Introduction%20to%20Secure%20DevOps/#sql-injection-attack","title":"SQL injection attack","text":"<p>SQL Injection is an attack that makes it possible to execute malicious SQL statements. These statements control a database server behind a web application.</p> <p>Attackers can use SQL Injection vulnerabilities to bypass application security measures. They can go around authentication and authorization of a web page or web application and retrieve the content of the entire SQL database. They can also use SQL Injection to add, modify, and delete records in the database.</p>"},{"location":"Implement-security-and-validate-code-bases-for-compliance/Introduction%20to%20Secure%20DevOps/#devsecops-pipeline","title":"DevSecOps pipeline","text":"<p>Two essential features of Secure DevOps Pipelines that aren't found in standard DevOps Pipelines are:</p> <ul> <li>Package management and the approval process associated with it. The previous workflow diagram details other steps for adding software packages to the Pipeline and the approval processes that packages must go through before they're used. These steps should be enacted early in the Pipeline to identify issues sooner in the cycle.</li> <li>Source Scanner is also an extra step for scanning the source code. This step allows for security scanning and checking for vulnerabilities that aren't present in the application code. The scanning occurs after the app is built before release and pre-release testing. Source scanning can identify security vulnerabilities earlier in the cycle.</li> </ul>"},{"location":"Implement-security-and-validate-code-bases-for-compliance/Introduction%20to%20Secure%20DevOps/#continuous-security-validation","title":"Continuous security validation","text":"<p>Several tools can be used for it:</p> <ul> <li>SonarQube.</li> <li>Visual Studio Code Analysis and the Roslyn Security Analyzers.</li> <li>Checkmarx: A Static Application Security Testing (SAST) tool.</li> <li>BinSkim: A binary static analysis tool that provides security and correctness results for Windows portable executables and many more.</li> </ul>"},{"location":"Implement-security-and-validate-code-bases-for-compliance/OWASP%20and%20dynamic%20analyzers/","title":"OWASP and dynamic analyzers","text":""},{"location":"Implement-security-and-validate-code-bases-for-compliance/OWASP%20and%20dynamic%20analyzers/#owasp-secure-coding-practices","title":"OWASP Secure Coding Practices","text":"<p>The starting point for secure development is to use secure-coding practices.</p> <p>The Open Web Application Security Project (OWASP) is a global charitable organization focused on improving software security.</p>"},{"location":"Implement-security-and-validate-code-bases-for-compliance/OWASP%20and%20dynamic%20analyzers/#owasp-zap-penetration-test","title":"OWASP ZAP penetration test","text":"<p>ZAP is a free penetration testing tool for beginners to professionals. ZAP includes an API and a weekly docker container image to integrate into your deployment process.</p> <p>The application CI/CD pipeline should run within a few minutes, so you don't want to include any long-running processes.</p> <p>The baseline scan is designed to identify vulnerabilities within a couple of minutes, making it a good option for the application CI/CD pipeline.</p> <p>The Nightly OWASP ZAP can spider the website and run the full-Active Scan to evaluate the most combinations of possible vulnerabilities.</p> <p>OWASP ZAP can be installed on any machine in your network, but we like to use the OWASP Zap/Weekly docker container within Azure Container Services.</p> <p>It allows for the latest updates to the image. It will enable the spin-up of multiple image instances so several applications within an enterprise can be scanned simultaneously.</p> <p>The following figure outlines the steps for the Application CI/CD pipeline and the longer-running Nightly OWASP ZAP pipeline.</p> <p></p>"},{"location":"Implement-security-and-validate-code-bases-for-compliance/Security%20monitoring%20and%20governance/","title":"Security monitoring and governance","text":""},{"location":"Implement-security-and-validate-code-bases-for-compliance/Security%20monitoring%20and%20governance/#pipeline-security","title":"Pipeline security","text":"<ul> <li>Authentication and authorization: Use multifactor authentication (MFA), even across internal domains, and just-in-time administration tools such as Azure PowerShell Just Enough Administration (JEA), to protect against privilege escalations.  </li> <li>The CI/CD Release Pipeline: If the release pipeline and cadence are damaged, use this pipeline to rebuild infrastructure. Manage Infrastructure as Code (IaC) with Azure Resource Manager or use the Azure platform as a service (PaaS) or a similar service. Your pipeline will automatically create new instances and then destroy them. It limits the places where attackers can hide malicious code inside your infrastructure. Azure DevOps will encrypt the secrets in your pipeline. As a best practice, rotate the passwords just as you would with other credentials.</li> <li>Permissions management: You can manage permissions to secure the pipeline with role-based access control (RBAC), just as you would for your source code. It keeps you in control of editing the build and releases definitions that you use for production.</li> <li>Dynamic scanning: It's the process of testing the running application with known attack patterns. You could implement penetration testing as part of your release.</li> <li>Production monitoring: It's a critical DevOps practice. The specialized services for detecting anomalies related to intrusion are known as Security Information and Event Management. </li> </ul>"},{"location":"Implement-security-and-validate-code-bases-for-compliance/Security%20monitoring%20and%20governance/#microsoft-defender-for-cloud","title":"Microsoft Defender for Cloud","text":"<p>Microsoft Defender for Cloud is a monitoring service that provides threat protection across all your services both in Azure and on-premises. Microsoft Defender can:</p> <ul> <li>Provide security recommendations based on your configurations, resources, and networks.</li> <li>Monitor security settings across on-premises and cloud workloads and automatically apply required security to new services as they come online.</li> <li>Continuously monitor all your services and do automatic security assessments to identify potential vulnerabilities before they can be exploited.</li> <li>Use Azure Machine Learning to detect and block malicious software from being installed on your virtual machines (VMs) and services. You can also define a list of allowed applications to ensure that only the validated apps can execute.</li> <li>Analyze and identify potential inbound attacks and help investigate threats and any post-breach activity that might have occurred.</li> <li>Provide just-in-time (JIT) access control for ports by reducing your attack surface by ensuring the network only allows traffic that you require.</li> </ul>"},{"location":"Implement-security-and-validate-code-bases-for-compliance/Security%20monitoring%20and%20governance/#azure-policy","title":"Azure Policy","text":"<p>Azure Policy is an Azure service that you can create, assign, and manage policies. Policies enforce different rules and effects over your Azure resources, ensuring that your resources stay compliant with your standards and SLAs.</p> <p>Azure Policy uses policies and initiatives to provide policy enforcement capabilities. Azure Policy evaluates your resources by scanning for resources that don't follow the policies you create.</p>"},{"location":"Implement-security-and-validate-code-bases-for-compliance/Security%20monitoring%20and%20governance/#cicd-pipeline-integration","title":"CI/CD pipeline integration","text":"<p>The Check Gate task is an example of an Azure policy that you can integrate with your DevOps CI/CD pipeline.</p> <p>Using Azure policies, Check gate provides security and compliance assessment on the resources with an Azure resource group or subscription that you can specify.</p> <p>Check gate is available as a Release pipeline deployment task.</p>"},{"location":"Implement-security-and-validate-code-bases-for-compliance/Security%20monitoring%20and%20governance/#policies","title":"Policies","text":"<p>A policy definition specifies the resources to be evaluated and the actions to take on them. For example, you could prevent VMs from deploying if exposed to a public IP address. You could also contain a specific hard disk for deploying VMs to control costs. Policies are defined in the JavaScript Object Notation (JSON) format.</p> <pre><code>{\n    \"properties\": {\n        \"mode\": \"all\",\n        \"parameters\": {\n            \"allowedLocations\": {\n                \"type\": \"array\",\n                \"metadata\": {\n                    \"description\": \"The list of locations that can be specified when deploying resources\",\n                    \"strongType\": \"location\",\n                    \"displayName\": \"Allowed locations\"\n                }\n            }\n        },\n        \"displayName\": \"Allowed locations\",\n        \"description\": \"This policy enables you to restrict the locations your organization can specify when deploying resources.\",\n        \"policyRule\": {\n            \"if\": {\n                \"not\": {\n                    \"field\": \"location\",\n                    \"in\": \"[parameters('allowedLocations')]\"\n                }\n            },\n            \"then\": {\n                \"effect\": \"deny\"\n            }\n        }\n    }\n}\n</code></pre> <p>The following list is example policy definitions:</p> <ul> <li>Allowed Storage Account SKUs (Deny): Determines if a storage account being deployed is within a set of SKU sizes. Its effect is to deny all storage accounts that don't adhere to the set of defined SKU sizes.</li> <li>Allowed Resource Type (Deny): Defines the resource types that you can deploy. Its effect is to deny all resources that aren't part of this defined list.</li> <li>Allowed Locations (Deny): Restricts the available locations for new resources. Its effect is used to enforce your geo-compliance requirements.</li> <li>Allowed Virtual Machine SKUs (Deny): Specify a set of virtual machine SKUs you can deploy.</li> <li>Add a tag to resources (Modify): Applies a required tag and its default value if the deploy request does not specify it.</li> <li>Not allowed resource types (Deny): Prevents a list of resource types from being deployed.</li> </ul>"},{"location":"Implement-security-and-validate-code-bases-for-compliance/Security%20monitoring%20and%20governance/#policy-assignment","title":"Policy assignment","text":"<p>Policy definitions, whether custom or built-in, need to be assigned. A policy assignment is a policy definition that has been assigned to a specific scope. Scopes can range from a management group to a resource group.</p> <p>Child resources will inherit any policy assignments applied to their parents.</p>"},{"location":"Implement-security-and-validate-code-bases-for-compliance/Security%20monitoring%20and%20governance/#remediation","title":"Remediation","text":"<p>Resources found not to follow a deployIfNotExists or modify policy condition can be put into a compliant state through Remediation.</p> <p>Remediation instructs Azure Policy to run the deployIfNotExists effect or the tag operations of the policy on existing resources.</p>"},{"location":"Implement-security-and-validate-code-bases-for-compliance/Security%20monitoring%20and%20governance/#initiatives","title":"Initiatives","text":"<p>Initiatives work alongside policies in Azure Policy. An initiative definition is a set of policy definitions to help track your compliance state for meeting large-scale compliance goals. Even if you have a single policy, we recommend using initiatives if you anticipate increasing your number of policies over time. Applying an initiative definition to a specific scope is called an initiative assignment.</p> <p>Initiative definitions simplify the process of managing and assigning policy definitions by grouping sets of policies into a single item.</p> <p>For example, you can create an initiative named Enable Monitoring in Azure Security Center to monitor security recommendations from Azure Security Center.</p> <p>Under this example initiative, you would have the following policy definitions:</p> <ul> <li>Monitor unencrypted SQL Database in Security Center. This policy definition monitors unencrypted SQL databases and servers.</li> <li>Monitor OS vulnerabilities in Security Center. This policy definition monitors servers that don't satisfy a specified OS baseline configuration.</li> <li>Monitor missing Endpoint Protection in Security Center. This policy definition monitors servers without an endpoint protection agent installed.</li> </ul>"},{"location":"Implement-security-and-validate-code-bases-for-compliance/Security%20monitoring%20and%20governance/#resource-locks","title":"Resource locks","text":"<p>Locks help you prevent accidental deletion or modification of your Azure resources. You can manage locks from within the Azure portal.</p> <p>In the Azure portal, locks are called <code>Delete</code> and <code>Read-only</code>, respectively.</p> <p>You might need to lock a subscription, resource group, or resource to prevent users from accidentally deleting or modifying critical resources.</p> <p>You can set a lock level to <code>CanNotDelete</code> or <code>ReadOnly</code>:</p> <ul> <li><code>CanNotDelete</code> means that authorized users can read and modify a resource, but they can't delete it.</li> <li><code>ReadOnly</code> means that authorized users can read a resource, but they can't modify or delete it.</li> </ul>"},{"location":"Implement-security-and-validate-code-bases-for-compliance/Security%20monitoring%20and%20governance/#azure-blueprints","title":"Azure Blueprints","text":"<p>Azure Blueprints enables cloud architects to define a repeatable set of Azure resources that implement and adhere to an organization's standards, patterns, and requirements.</p> <p>Azure Blueprints helps development teams build and deploy new environments rapidly with a set of built-in components that speed up development and delivery.</p> <p>Azure Blueprints provides a declarative way to orchestrate deployment for various resource templates and artifacts, including:</p> <ul> <li>Role assignments.</li> <li>Policy assignments.</li> <li>Azure Resource Manager templates.</li> <li>Resource groups.</li> </ul> <p>To implement Azure Blueprints, complete the following high-level steps:</p> <ol> <li>Create a blueprint.</li> <li>Assign the blueprint.</li> <li>Track the blueprint assignments.</li> </ol> <p>With Azure Blueprints, the relationship between the blueprint definition (what should be deployed) and the blueprint assignment (what is deployed) is preserved.</p>"},{"location":"Implement-security-and-validate-code-bases-for-compliance/Software%20Composition%20Analysis/","title":"Software Composition Analysis","text":""},{"location":"Implement-security-and-validate-code-bases-for-compliance/Software%20Composition%20Analysis/#software-composition-analysis_1","title":"Software composition analysis","text":""},{"location":"Implement-security-and-validate-code-bases-for-compliance/Software%20Composition%20Analysis/#package-management","title":"Package management","text":"<p>Just as teams use version control as a single source of truth for source code, Secure DevOps relies on a package manager as the unique source of binary components.</p> <p>Using binary package management, a development team can create a local cache of approved components and a trusted feed for the Continuous Integration (CI) pipeline.</p>"},{"location":"Implement-security-and-validate-code-bases-for-compliance/Software%20Composition%20Analysis/#the-role-of-oss-components","title":"The Role of OSS components","text":"<p>Development work is more productive because of the wide availability of reusable Open-source software (OSS) components. However, OSS component reuse comes with the risk that reused dependencies can have security vulnerabilities. As a result, many users find security vulnerabilities in their applications because of the Node.js package versions they consume.</p>"},{"location":"Implement-security-and-validate-code-bases-for-compliance/Software%20Composition%20Analysis/#integrating-mend-with-azure-pipelines","title":"Integrating Mend with Azure Pipelines","text":"<p>The Mend extension is available on the Azure DevOps Marketplace. Using Mend, you can integrate extensions with your CI/CD pipeline to address Secure DevOps security-related issues. The Mend extension specifically addresses open-source security, quality, and license compliance concerns for a team consuming external packages. </p> <ul> <li>Continuously detect all open-source components in your software.</li> <li>Receive alerts on open-source security vulnerabilities.</li> <li>Automatically enforce open-source security and license compliance policies.</li> </ul>"},{"location":"Implement-security-and-validate-code-bases-for-compliance/Software%20Composition%20Analysis/#github-dependabot-alerts-and-security-updates","title":"GitHub Dependabot alerts and security updates","text":"<p>GitHub Dependabot detects vulnerable dependencies and sends Dependabot alerts about them in several situations:</p> <ul> <li>A new vulnerability is added to the GitHub Advisory database.</li> <li>New vulnerability data from Mend is processed.</li> <li>Dependency graph for a repository changes.</li> </ul> <p>Alerts are detected in public repositories by default but can be enabled for other repositories.</p> <p>Another key advantage of Dependabot security updates is that they can automatically create pull requests. A developer can then review the suggested update and triage what is required to incorporate it.</p>"},{"location":"Implement-security-and-validate-code-bases-for-compliance/Software%20Composition%20Analysis/#integrating-software-composition-analysis-checks-into-pipelines","title":"Integrating software composition analysis checks into pipelines","text":""},{"location":"Implement-security-and-validate-code-bases-for-compliance/Software%20Composition%20Analysis/#pull-request-code-scan-analysis-integration","title":"Pull request code scan analysis integration","text":"<p>DevOps teams can submit proposed changes to an application's (main) codebase using pull requests (PRs). To avoid introducing new issues, developers need to verify the effects of the code changes before creating a PR. A PR is typically made for each small change in a DevOps process. Changes are continuously merged with the main codebase to keep the main codebase up to date. Ideally, a developer should check for security issues before creating a PR.</p> <p>Azure Marketplace extensions that help integrate scans during PRs include:</p> <ul> <li>Mend: Helps validate dependencies with its binary fingerprinting.</li> <li>Checkmarx: Provides an incremental scan of changes.</li> <li>Veracode: Implements the concept of a developer sandbox.</li> <li>Black Duck by Synopsis: An auditing tool for open-source code to help identify, fix, and manage compliance.</li> </ul>"},{"location":"Implement-security-and-validate-code-bases-for-compliance/Software%20Composition%20Analysis/#build-and-release-definition-code-scan-analysis-and-integration","title":"Build and release definition code scan, analysis, and integration","text":"<p>Developers need to optimize CI for speed to get immediate feedback about build issues. Code scanning can be performed quickly enough to integrate the CI build definition, preventing a broken build. It enables developers to restore a build's status to ready/ green by fixing potential issues immediately.</p> <p>At the same time, the CD needs to be thorough. In Azure DevOps, the CD is typically managed through release definitions (which progress the build output across environments) or other build definitions.</p> <p>Build definitions can be scheduled (daily) or triggered with each commit. In either case, the build definition can do a longer static analysis scan (as illustrated in the following image).</p> <p></p>"},{"location":"Implement-security-and-validate-code-bases-for-compliance/Software%20Composition%20Analysis/#inspect-packages-in-the-delivery-pipeline","title":"Inspect packages in the delivery pipeline","text":"Tool Type Artifactory Artifact repository. SonarQube A static code analysis tool. Mend (Bolt) Build scanning."},{"location":"Implement-security-and-validate-code-bases-for-compliance/Software%20Composition%20Analysis/#lab-address","title":"Lab address","text":"<p>https://aka.ms/az-400-implement-security-and-compliance-azure-devops-pipeline</p>"},{"location":"Implement-security-and-validate-code-bases-for-compliance/Static%20analyzers/","title":"Static analyzers","text":""},{"location":"Implement-security-and-validate-code-bases-for-compliance/Static%20analyzers/#sonarcloud","title":"SonarCloud","text":"<p>SonarQube is an open-source platform that is the de facto solution for understanding and managing technical debt.</p>"},{"location":"Implement-security-and-validate-code-bases-for-compliance/Static%20analyzers/#codeql-in-github","title":"CodeQL in GitHub","text":"<p>Developers use CodeQL to automate security checks. CodeQL treats code like data that can be queried. GitHub researchers and community researchers have contributed standard CodeQL queries, and you can write your own.</p> <p>A CodeQL analysis consists of three phases:</p> <ul> <li>Creating a CodeQL database (based upon the code).</li> <li>Run CodeQL queries against the database.</li> <li>Interpret the results.</li> <li>CodeQL is available as a command-line interpreter and an extension for Visual Studio Code.</li> </ul>"},{"location":"Implement-security-and-validate-code-bases-for-compliance/Static%20analyzers/#lab-address","title":"Lab address","text":"<p>https://aka.ms/az-400-manage-technical-debt-with-sonarqube-and-azure-devops</p>"},{"location":"Manage-infrastructure-as-code-using-Azure-and-DSC/Azure%20Automation%20with%20DevOps/","title":"Azure Automation with DevOps","text":""},{"location":"Manage-infrastructure-as-code-using-Azure-and-DSC/Azure%20Automation%20with%20DevOps/#automation-accounts","title":"Automation Accounts","text":"<p>Automation accounts are like Azure Storage accounts in that they serve as a container to store automation artifacts. These artifacts could be a container for all your runbooks, runbook executions (jobs), and the assets on which your runbooks depend. An Automation account gives you access to managing all Azure resources via an API. To safeguard it, the Automation account creation requires subscription-owner access.</p>"},{"location":"Manage-infrastructure-as-code-using-Azure-and-DSC/Azure%20Automation%20with%20DevOps/#runbooks","title":"Runbooks","text":"<p>Runbooks serve as repositories for your custom scripts and workflows. They also typically reference Automation shared resources such as credentials, variables, connections, and certificates. Runbooks can also contain other runbooks, allowing you to build more complex workflows. You can invoke and run runbooks either on-demand or according to a schedule by using Automation Schedule assets.</p>"},{"location":"Manage-infrastructure-as-code-using-Azure-and-DSC/Azure%20Automation%20with%20DevOps/#types-of-runbooks","title":"Types of runbooks","text":"<ul> <li>Graphical runbooks: Graphical runbooks and Graphical PowerShell Workflow runbooks are created and edited with the graphic editor in the Azure portal.</li> <li>PowerShell runbooks:PowerShell runbooks are based on Windows PowerShell. You edit the runbook code directly, using the text editor in the Azure portal.</li> <li>PowerShell Workflow runbooks: PowerShell Workflow runbooks are text runbooks based on Windows PowerShell Workflow. PowerShell Workflow runbooks use parallel processing to allow for simultaneous completion of multiple tasks. Workflow runbooks take longer to start than PowerShell runbooks because they must be compiled before running.</li> <li>Python runbooks: Python runbooks compile under Python 2. You can directly edit the code of the runbook using the text editor in the Azure portal, or you can use any offline text editor and import the runbook into Azure Automation. You can also use Python libraries. However, only Python 2 is supported at this time. To use third-party libraries, you must first import the package into the Automation Account.</li> </ul>"},{"location":"Manage-infrastructure-as-code-using-Azure-and-DSC/Azure%20Automation%20with%20DevOps/#runbook-gallery","title":"Runbook gallery","text":"<ol> <li>Open your Automation account, and then select Process Automation &gt; Runbooks.</li> <li>In the runbooks pane, select Browse gallery.</li> <li>From the runbook gallery, locate the runbook item you want, select it, and select Import.</li> </ol>"},{"location":"Manage-infrastructure-as-code-using-Azure-and-DSC/Azure%20Automation%20with%20DevOps/#webhooks","title":"Webhooks","text":"<p>You can automate starting a runbook either by scheduling it or by using a webhook. A webhook allows you to start a particular runbook in Azure Automation through a single HTTPS request.</p> <p>It allows external services such as Azure DevOps, GitHub, or custom applications to start runbooks without implementing more complex solutions using the Azure Automation API.</p> <ol> <li>In the Azure portal, open the runbook that you want to create the webhook.</li> <li>In the runbook pane, under Resources, select Webhooks, and then choose + Add webhook.</li> <li>Select Create new webhook.</li> <li> <p>In the Create new webhook dialog, there are several values you need to configure. After you configure them, select Create:</p> <ul> <li>Name. Specify any name you want for a webhook because the name isn't exposed to the client. It's only used for you to identify the runbook in Azure Automation.</li> <li>Enabled. A webhook is enabled by default when it's created. If you set it to Disabled, then no client can use it.</li> <li>Expires. Each webhook has an expiration date, at which time it can no longer be used. You can continue to modify the date after creating the webhook providing the webhook isn't expired.</li> <li>URL. The webhook URL is the unique address that a client calls with an HTTP POST to start the runbook linked to the webhook. It's automatically generated when you create the webhook, and you can't specify a custom URL. The URL contains a security token that allows the runbook to be invoked by a third-party system with no further authentication. For this reason, treat it like a password. You can only view the URL in the Azure portal for security reasons when the webhook is created. Make a note of the URL in a secure location for future use.</li> <li> <p>Select the Parameters run settings (Default: Azure) option. This option has the following characteristics, which allows you to complete the following actions:</p> </li> <li> <p>If the runbook has mandatory parameters, you'll need to provide these required parameters during creation. You aren't able to create the webhook unless values are provided.</p> </li> <li>If there are no mandatory parameters in the runbook, there's no configuration required here.</li> <li>The webhook must include values for any mandatory parameters of the runbook and include values for optional parameters.</li> <li>When a client starts a runbook using a webhook, it can't override the parameter values defined.</li> <li>To receive data from the client, the runbook can accept a single parameter called <code>$WebhookData</code> of type <code>[object]</code> that contains data that the client includes in the POST request.</li> <li>There's no required webhook configuration to support the <code>$WebhookData</code> parameter.</li> </ul> </li> </ol>"},{"location":"Manage-infrastructure-as-code-using-Azure-and-DSC/Azure%20Automation%20with%20DevOps/#automation-and-shared-resources","title":"Automation and shared resources","text":"<p>Azure Automation contains shared resources that are globally available to be associated with or used in a runbook. There are currently eight shared resources categories:</p> <ul> <li>Schedules: It allows you to define a one-off or recurring schedule.</li> <li>Modules: Contains Azure PowerShell modules.</li> <li>Modules gallery: It allows you to identify and import PowerShell modules into your Azure automation account.</li> <li>Python packages: Allows you to import a Python package by uploading: .whl or tar.gz packages.</li> <li>Credentials: It allows you to create username and password credentials.</li> <li>Connections: It allows you to specify Azure, Azure classic certificate, or Azure Service principal connections.</li> <li>Certificates: It allows you to upload certificates in .cer or pfx format.</li> <li>Variables: It allows you to define encrypted or unencrypted variables of types\u2014for example, String, Boolean, DateTime, Integer, or no specific type.</li> </ul>"},{"location":"Manage-infrastructure-as-code-using-Azure-and-DSC/Azure%20Automation%20with%20DevOps/#powershell-workflows","title":"PowerShell Workflows","text":"<p>PowerShell Workflow lets IT pros and developers apply the benefits of Windows Workflow Foundation with the automation capabilities and ease of using Windows PowerShell.</p>"},{"location":"Manage-infrastructure-as-code-using-Azure-and-DSC/Azure%20Automation%20with%20DevOps/#activities","title":"Activities","text":"<p>An activity is a specific task that you want a workflow to do. Just as a script is composed of one or more commands, a workflow is composed of activities carried out in sequence.</p> <p>You can also use a script as a single command in another script and use a workflow as an activity within another workflow.</p>"},{"location":"Manage-infrastructure-as-code-using-Azure-and-DSC/Azure%20Automation%20with%20DevOps/#workflow-characteristics","title":"Workflow characteristics","text":"<ul> <li>Be long-running.</li> <li>Be repeated over and over.</li> <li>Run tasks in parallel.</li> <li>Be interrupted\u2014can be stopped and restarted, suspended, and resumed.</li> <li>Continue after an unexpected interruption, such as a network outage or computer/server restart.</li> </ul>"},{"location":"Manage-infrastructure-as-code-using-Azure-and-DSC/Azure%20Automation%20with%20DevOps/#checkpoint-and-parallel-processing-in-workflows","title":"Checkpoint and parallel processing in Workflows","text":""},{"location":"Manage-infrastructure-as-code-using-Azure-and-DSC/Azure%20Automation%20with%20DevOps/#checkpoint","title":"Checkpoint","text":"<p>A checkpoint is a snapshot of the current state of the workflow.</p> <p>Checkpoints include the current value for variables and any output generated up to that point. (For more information on what a checkpoint is, read the checkpoint webpage.)</p> <p>If a workflow ends in an error or is suspended, the next time it runs, it will start from its last checkpoint instead of at the beginning of the workflow.</p> <pre><code>&lt;Activity1&gt;\n        Checkpoint-Workflow\n            &lt;Activity2&gt;\n                &lt;Exception&gt;\n            &lt;Activity3&gt;\n</code></pre>"},{"location":"Manage-infrastructure-as-code-using-Azure-and-DSC/Azure%20Automation%20with%20DevOps/#parallel-processing","title":"Parallel processing","text":"<p>A script block has multiple commands that run concurrently (or in parallel) instead of sequentially, as for a typical script.</p> <p>It's referred to as parallel processing. (More information about parallel processing is available on the Parallel processing webpage.)</p> <p>In the following example, two <code>vm0</code> and <code>vm1</code> VMs will be started concurrently, and vm2 will only start after vm0 and <code>vm1</code> have started.</p> <pre><code>Parallel\n    {\n        Start-AzureRmVM -Name $vm0 -ResourceGroupName $rg \n        Start-AzureRmVM -Name $vm1 -ResourceGroupName $rg\n    }\n\n    Start-AzureRmVM -Name $vm2 -ResourceGroupName $rg\n</code></pre> <p>Another parallel processing example would be the following constructs that introduce some extra options:</p> <p><code>ForEach -Parallel</code>. You can use the <code>ForEach -Parallel</code> construct to concurrently process commands for each item in a collection. The items in the collection are processed in parallel while the commands in the script block run sequentially.</p> <p>In the following example, <code>Activity1</code> starts at the same time for all items in the collection.</p> <p>For each item, <code>Activity2</code> starts after <code>Activity1</code> completes. Activity3 starts only after both <code>Activity1</code> and <code>Activity2</code> have been completed for all items.</p> <p><code>ThrottleLimit</code> - We use the <code>ThrottleLimit</code> parameter to limit parallelism. Too high of a <code>ThrottleLimit</code> can cause problems. The ideal value for the <code>ThrottleLimit</code> parameter depends on several environmental factors. Try starting with a low <code>ThrottleLimit</code> value, and then increase the value until you find one that works for your specific circumstances:</p> <pre><code>ForEach -Parallel -ThrottleLimit 10 ($&lt;item&gt; in $&lt;collection&gt;)\n{\n    &lt;Activity1&gt;\n    &lt;Activity2&gt;\n}\n&lt;Activity3&gt;\n</code></pre>"},{"location":"Manage-infrastructure-as-code-using-Azure-and-DSC/Create%20Azure%20resources%20by%20using%20Azure%20CLI/","title":"Azure resources by using Azure CLI","text":""},{"location":"Manage-infrastructure-as-code-using-Azure-and-DSC/Create%20Azure%20resources%20by%20using%20Azure%20CLI/#azure-cli","title":"Azure CLI","text":"<p>Azure CLI is a command-line program you use to connect to Azure and execute administrative commands on Azure resources. It runs on Linux, macOS, and Windows operating systems.</p> <p>For example, to restart a VM, you would use a command such as:</p> <pre><code>az vm restart -g MyResourceGroup -n MyVm\n</code></pre>"},{"location":"Manage-infrastructure-as-code-using-Azure-and-DSC/Create%20Azure%20resources%20by%20using%20Azure%20CLI/#working-with-azure-cli","title":"Working with Azure CLI","text":"<p>Commands in CLI are structured in groups and subgroups. Each group represents a service provided by Azure, and the subgroups divide commands for these services into logical groupings.</p>"},{"location":"Manage-infrastructure-as-code-using-Azure-and-DSC/Create%20Azure%20resources%20by%20using%20Azure%20CLI/#finding-commands","title":"Finding commands","text":"<p>For example, if you want to find commands that might help you manage a storage blob, you can use the following find command:</p> <pre><code>az find blob\n</code></pre> <p>If you know the command's name you want, the help argument for that command will get you more detailed information on the command\u2014also, a list of the available subcommands for a command group.</p> <pre><code>az storage blob --help\n</code></pre>"},{"location":"Manage-infrastructure-as-code-using-Azure-and-DSC/Create%20Azure%20resources%20by%20using%20Azure%20CLI/#creating-resources","title":"Creating resources","text":"<p>1. Connecting</p> <pre><code>az login\n</code></pre> <p>2. Creating resources</p> <p>You'll often need to create a new resource group before you create a new Azure service. So we'll use resource groups as an example to show how to create Azure resources from the Azure CLI. The Azure CLI group create command creates a resource group.</p> <pre><code>az group create --name &lt;name&gt; --location &lt;location&gt;\n</code></pre> <p>3. Veryfing installation</p> <p>For many Azure resources, Azure CLI provides a list subcommand to get resource details.</p> <pre><code>az group list\n</code></pre> <p>To get more concise information, you can format the output as a simple table.</p> <p>```bash az group list --output table ``````</p>"},{"location":"Manage-infrastructure-as-code-using-Azure-and-DSC/Creating%20Azure%20resources%20using%20Azure%20Resource%20Manager%20templates/","title":"Creating Azure resources using Azure Resource Manager templates","text":""},{"location":"Manage-infrastructure-as-code-using-Azure-and-DSC/Creating%20Azure%20resources%20using%20Azure%20Resource%20Manager%20templates/#azure-resource-manager-templates","title":"Azure Resource Manager templates","text":"<p>Using Resource Manager templates will make your deployments faster and more repeatable.</p> <ul> <li>Templates improve consistency. Resource Manager templates provide a common language for you and others to describe your deployments. Despite the tool or SDK that you use to deploy the template, the template's structure, format, and expressions remain the same.</li> <li>Templates help express complex deployments. Templates enable you to deploy multiple resources in the correct order. For example, you wouldn't want to deploy a VM before creating an operating system (OS) disk or network interface. Resource Manager maps out each resource and its dependent resources and creates dependent resources first. Dependency mapping helps ensure that the deployment is carried out in the correct order.</li> <li>Templates reduce manual, error-prone tasks. Manually creating and connecting resources can be time-consuming, and it's easy to make mistakes. The Resource Manager ensures that the deployment happens the same way every time.</li> <li>Templates are code. Templates express your requirements through code. Think of a template as a type of Infrastructure as Code that can be shared, tested, and versioned like any other piece of software. Also, because templates are code, you can create a record that you can follow. The template code documents the deployment. Also, most users maintain their templates under revision control, such as GIT. Its revision history also records how the template (and your deployment) has evolved when you change the template.</li> <li>Templates promote reuse. Your template can contain parameters that are filled in when the template runs. A parameter can define a username or password, a domain name, and other necessary items. Template parameters also enable you to create multiple versions of your infrastructure, such as staging and production, while still using the same template.</li> <li>Templates are linkable. You can link Resource Manager templates together to make the templates themselves modular. You can write small templates that define a solution and then combine them to create a complete system.</li> </ul> <p>Azure Resource Manager templates are written in JSON, which allows you to express data stored as an object (such as a virtual machine) in text.</p>"},{"location":"Manage-infrastructure-as-code-using-Azure-and-DSC/Creating%20Azure%20resources%20using%20Azure%20Resource%20Manager%20templates/#parameters","title":"Parameters","text":"<p>For example, you might allow template users to set a username, password, or domain name.</p> <pre><code>\"parameters\": {\n  \"adminUsername\": {\n    \"type\": \"string\",\n    \"metadata\": {\n      \"description\": \"Username for the Virtual Machine.\"\n    }\n  },\n  \"adminPassword\": {\n    \"type\": \"securestring\",\n    \"metadata\": {\n      \"description\": \"Password for the Virtual Machine.\"\n    }\n  }\n}\n</code></pre>"},{"location":"Manage-infrastructure-as-code-using-Azure-and-DSC/Creating%20Azure%20resources%20using%20Azure%20Resource%20Manager%20templates/#variables","title":"Variables","text":"<p>For example, you might define a storage account name one time as a variable and then use that variable throughout the template.</p> <pre><code>\"variables\": {\n  \"nicName\": \"myVMNic\",\n  \"addressPrefix\": \"10.0.0.0/16\",\n  \"subnetName\": \"Subnet\",\n  \"subnetPrefix\": \"10.0.0.0/24\",\n  \"publicIPAddressName\": \"myPublicIP\",\n  \"virtualNetworkName\": \"MyVNET\"\n}\n</code></pre>"},{"location":"Manage-infrastructure-as-code-using-Azure-and-DSC/Creating%20Azure%20resources%20using%20Azure%20Resource%20Manager%20templates/#functions","title":"Functions","text":"<p>An example that creates a function for creating a unique name to use when creating resources that have globally unique naming requirement.</p> <pre><code>\"functions\": [\n  {\n    \"namespace\": \"contoso\",\n    \"members\": {\n      \"uniqueName\": {\n        \"parameters\": [\n          {\n            \"name\": \"namePrefix\",\n            \"type\": \"string\"\n          }\n        ],\n        \"output\": {\n          \"type\": \"string\",\n          \"value\": \"[concat(toLower(parameters('namePrefix')), uniqueString(resourceGroup().id))]\"\n        }\n      }\n    }\n  }\n],\n</code></pre>"},{"location":"Manage-infrastructure-as-code-using-Azure-and-DSC/Creating%20Azure%20resources%20using%20Azure%20Resource%20Manager%20templates/#resources","title":"Resources","text":"<p>This section is where you define the Azure resources that make up your deployment. An example that creates a public IP address resource.</p> <pre><code>{\n  \"type\": \"Microsoft.Network/publicIPAddresses\",\n  \"name\": \"[variables('publicIPAddressName')]\",\n  \"location\": \"[parameters('location')]\",\n  \"apiVersion\": \"2018-08-01\",\n  \"properties\": {\n    \"publicIPAllocationMethod\": \"Dynamic\",\n    \"dnsSettings\": {\n      \"domainNameLabel\": \"[parameters('dnsLabelPrefix')]\"\n    }\n  }\n}\n</code></pre>"},{"location":"Manage-infrastructure-as-code-using-Azure-and-DSC/Creating%20Azure%20resources%20using%20Azure%20Resource%20Manager%20templates/#outputs","title":"Outputs","text":"<p>This section is where you define any information you'd like to receive when the template runs. For example, you might want to receive your VM's IP address or fully qualified domain name (FQDN), the information you won't know until the deployment runs.</p> <pre><code>\"outputs\": {\n  \"hostname\": {\n    \"type\": \"string\",\n    \"value\": \"[reference(variables('publicIPAddressName')).dnsSettings.fqdn]\"\n  }\n}\n</code></pre>"},{"location":"Manage-infrastructure-as-code-using-Azure-and-DSC/Creating%20Azure%20resources%20using%20Azure%20Resource%20Manager%20templates/#managing-dependencies","title":"Managing dependencies","text":"<p>For any given resource, other resources might need to exist before you can deploy the resource.</p> <p>Within your template, the <code>dependsOn</code> element enables you to define one resource dependent on one or more other resources.</p>"},{"location":"Manage-infrastructure-as-code-using-Azure-and-DSC/Creating%20Azure%20resources%20using%20Azure%20Resource%20Manager%20templates/#circular-dependencies","title":"Circular dependencies","text":"<p>A circular dependency is a problem with dependency sequencing, resulting in the deployment going around in a loop and unable to continue. As a result, the Resource Manager can't deploy the resources. Resource Manager identifies circular dependencies during template validation.</p>"},{"location":"Manage-infrastructure-as-code-using-Azure-and-DSC/Creating%20Azure%20resources%20using%20Azure%20Resource%20Manager%20templates/#modularizing-templates","title":"Modularizing templates","text":"<p>When using Azure Resource Manager templates, it's best to modularize them by breaking them into individual components. The primary methodology to use is by using linked templates.</p>"},{"location":"Manage-infrastructure-as-code-using-Azure-and-DSC/Creating%20Azure%20resources%20using%20Azure%20Resource%20Manager%20templates/#linked-template","title":"Linked template","text":"<pre><code>\"resources\": [\n  {\n      \"apiVersion\": \"2017-05-10\",\n      \"name\": \"linkedTemplate\",\n      \"type\": \"Microsoft.Resources/deployments\",\n      \"properties\": {\n          \"mode\": \"Incremental\",\n          &lt;link-to-external-template&gt;\n      }\n  }\n]\n</code></pre>"},{"location":"Manage-infrastructure-as-code-using-Azure-and-DSC/Creating%20Azure%20resources%20using%20Azure%20Resource%20Manager%20templates/#nested-template","title":"Nested template","text":"<p>You can also nest a template within the main template, use the template property, and specify the template syntax.</p> <pre><code>\"resources\": [\n  {\n    \"apiVersion\": \"2017-05-10\",\n    \"name\": \"nestedTemplate\",\n    \"type\": \"Microsoft.Resources/deployments\",\n    \"properties\": {\n      \"mode\": \"Incremental\",\n      \"template\": {\n        \"$schema\": \"https://schema.management.azure.com/schemas/2015-01-01/deploymentTemplate.json#\",\n        \"contentVersion\": \"1.0.0.0\",\n        \"resources\": [\n          {\n            \"type\": \"Microsoft.Storage/storageAccounts\",\n            \"name\": \"[variables('storageName')]\",\n            \"apiVersion\": \"2015-06-15\",\n            \"location\": \"West US\",\n            \"properties\": {\n              \"accountType\": \"Standard_LRS\"\n            }\n          }\n        ]\n      }\n    }\n  }\n]\n</code></pre>"},{"location":"Manage-infrastructure-as-code-using-Azure-and-DSC/Creating%20Azure%20resources%20using%20Azure%20Resource%20Manager%20templates/#external-templates-and-parameters","title":"External templates and parameters","text":"<p>To link to an external template and parameter file, use <code>templateLink</code> and <code>parametersLink</code>. When linking to a template, ensure that the Resource Manager service can access it.</p> <pre><code>\"resources\": [\n    {\n      \"name\": \"linkedTemplate\",\n      \"type\": \"Microsoft.Resources/deployments\",\n      \"apiVersion\": \"2018-05-01\",\n      \"properties\": {\n          \"mode\": \"Incremental\",\n          \"templateLink\": {\n              \"uri\":\"https://linkedtemplateek1store.blob.core.windows.net/linkedtemplates/linkedStorageAccount.json?sv=2018-03-28&amp;sr=b&amp;sig=dO9p7XnbhGq56BO%2BSW3o9tX7E2WUdIk%2BpF1MTK2eFfs%3D&amp;se=2018-12-31T14%3A32%3A29Z&amp;sp=r\"\n          },\n          \"parameters\": {\n              \"storageAccountName\":{\"value\": \"[variables('storageAccountName')]\"},\n              \"location\":{\"value\": \"[parameters('location')]\"}\n          }\n      }\n    },\n</code></pre>"},{"location":"Manage-infrastructure-as-code-using-Azure-and-DSC/Creating%20Azure%20resources%20using%20Azure%20Resource%20Manager%20templates/#deployment-modes","title":"Deployment modes","text":"<p>When deploying your resources using templates, you have three options:</p> <ul> <li>Validate: This option compiles the templates, validates the deployment, ensures the template is functional (for example, no circular dependencies), and correct syntax.</li> <li>Incremental mode (default): This option only deploys whatever is defined in the template. It doesn't remove or modify any resources that aren't defined in the template. For example, if you've deployed a VM via template and then renamed the VM in the template, the first VM deployed will remain after the template is rerun. It's the default mode.</li> <li>Complete mode: Resource Manager deletes resources that exist in the resource group but isn't specified in the template. For example, only resources defined in the template will be present in the resource group after the template deploys. As a best practice, use this mode for production environments to achieve idempotency in your deployment templates.</li> </ul>"},{"location":"Manage-infrastructure-as-code-using-Azure-and-DSC/Creating%20Azure%20resources%20using%20Azure%20Resource%20Manager%20templates/#managing-secrets-in-templates","title":"Managing secrets in templates","text":"<p>When passing a secure value (such as a password) as a parameter during deployment, you can retrieve the value from an Azure Key Vault. Reference the Key Vault and secret in your parameter file. The value is never exposed because you only reference its Key Vault ID. The Key Vault can exist in a different subscription than the resource group you're deploying.</p>"},{"location":"Manage-infrastructure-as-code-using-Azure-and-DSC/Creating%20Azure%20resources%20using%20Azure%20Resource%20Manager%20templates/#lab-address","title":"Lab address","text":"<p>https://microsoftlearning.github.io/AZ400-DesigningandImplementingMicrosoftDevOpsSolutions/Instructions/Labs/AZ400_M06_L12_Azure_Deployments_Using_Resource_Manager_Templates.html</p>"},{"location":"Manage-infrastructure-as-code-using-Azure-and-DSC/IaaC%20and%20configuration%20management/","title":"IaaC and configuration management","text":""},{"location":"Manage-infrastructure-as-code-using-Azure-and-DSC/IaaC%20and%20configuration%20management/#infrastructure-as-a-code","title":"Infrastructure as a Code","text":"Manual deployment Infrastructure as code Snowflake servers. A consistent server between environments. Deployment steps vary by environment. Environments are created or scaled easily. More verification steps and more elaborate manual processes. Fully automated creation of environment Updates. Increased documentation to account for differences. Transition to immutable infrastructure. Deployment on weekends to allow time to recover from errors. Use blue/green deployments. Slower release cadence to minimize pain and long weekends. Treat servers as cattle, not pets."},{"location":"Manage-infrastructure-as-code-using-Azure-and-DSC/IaaC%20and%20configuration%20management/#benefits-of-iaac","title":"Benefits of IaaC","text":"<ul> <li>Promotes auditing by making it easier to trace what was deployed, when, and how. (In other words, it improves traceability.)</li> <li>Provides consistent environments from release to release.</li> <li>Greater consistency across development, test, and production environments.</li> <li>Automates scale-up and scale-out processes.</li> <li>Allows configurations to be version-controlled.</li> <li>Provides code review and unit-testing capabilities to help manage infrastructure changes.</li> <li>Uses immutable service processes, meaning if a change is needed to an environment, a new service is deployed, and the old one was taken down, it isn't updated.</li> <li>Allows blue/green deployments. This release methodology minimizes downtime where two identical environments exist\u2014one is live, and the other isn't. Updates are applied to the server that isn't live. When testing is verified and complete, it's swapped with the different live servers. It becomes the new live environment, and the previous live environment is no longer the live.</li> <li>Treats infrastructure as a flexible resource that can be provisioned, de-provisioned, and reprovisioned as and when needed.</li> </ul>"},{"location":"Manage-infrastructure-as-code-using-Azure-and-DSC/IaaC%20and%20configuration%20management/#environment-configuration","title":"Environment configuration","text":"<p>Configuration management refers to automated configuration management, typically in version-controlled scripts, for an application and all the environments needed to support it.</p> Manual configuration Configuration as code Configuration bugs are challenging to identify. Bugs are easily reproducible. Error-prone. Consistent configuration. More verification steps and more elaborate manual processes. Increase deployment cadence to reduce the amount of incremental change. Increased documentation. Treat environment and configuration as executable documentation. Deployment on weekends to allow time to recover from errors. - Slower release cadence to minimize the requirement for long weekends. -"},{"location":"Manage-infrastructure-as-code-using-Azure-and-DSC/IaaC%20and%20configuration%20management/#benefits-of-configuration-management","title":"Benefits of configuration management","text":"<ul> <li>Bugs are more easily reproduced, audit help, and improve traceability.</li> <li>Provides consistency across environments such as dev, test, and release.</li> <li>It increased deployment cadence.</li> <li>Less documentation is needed and needs to be maintained as all configuration is available in scripts.</li> <li>Enables automated scale-up and scale out.</li> <li>Allows version-controlled configuration.</li> <li>Helps detect and correct configuration drift.</li> <li>Provides code-review and unit-testing capabilities to help manage infrastructure changes.</li> <li>Treats infrastructure as a flexible resource.</li> <li>Promotes automation.</li> </ul>"},{"location":"Manage-infrastructure-as-code-using-Azure-and-DSC/IaaC%20and%20configuration%20management/#declarative-vs-imperative-configuration","title":"Declarative vs. imperative configuration","text":"<ul> <li>Declarative (functional): The declarative approach states what the final state should be. When run, the script or definition will initialize or configure the machine to have the finished state declared without defining how that final state should be achieved.</li> <li>Imperative (procedural): In the imperative approach, the script states the how for the final state of the machine by executing the steps to get to the finished state. It defines what the final state needs to be but also includes how to achieve that final state. It also can consist of coding concepts such as for, *if-then, loops, and matrices.</li> </ul>"},{"location":"Manage-infrastructure-as-code-using-Azure-and-DSC/IaaC%20and%20configuration%20management/#idempotent-configuration","title":"Idempotent  configuration","text":"<p>Idempotence is a mathematical term that can be used in Infrastructure as Code and Configuration as Code. It can apply one or more operations against a resource, resulting in the same outcome.</p> <p>For example, running a script on a system should have the same outcome despite the number of times you execute the script. It shouldn't error out or do the same actions irrespective of the environment's starting state.</p> <p>In essence, if you apply a deployment to a set of resources 1,000 times, you should end up with the same result after each application of the script or template.</p>"},{"location":"Manage-infrastructure-as-code-using-Azure-and-DSC/Implementing%20Bicep/","title":"Implementing Bicep","text":""},{"location":"Manage-infrastructure-as-code-using-Azure-and-DSC/Implementing%20Bicep/#bicep-introduction","title":"Bicep introduction","text":"<p>Azure Bicep is the next revision of ARM templates designed to solve some of the issues developers were facing when deploying their resources to Azure. It's an Open Source tool and, in fact, a domain-specific language (DSL) that provides a means to declaratively codify infrastructure, which describes the topology of cloud resources such as VMs, Web Apps, and networking interfaces. It also encourages code reuse and modularity in designing the infrastructure as code files.</p>"},{"location":"Manage-infrastructure-as-code-using-Azure-and-DSC/Implementing%20Bicep/#installing-bicep","title":"Installing Bicep","text":"<pre><code># for new installation\naz bicep install\n\n# upgrading existing installation\naz bicep upgrade\n</code></pre>"},{"location":"Manage-infrastructure-as-code-using-Azure-and-DSC/Implementing%20Bicep/#deploying-bicep","title":"Deploying Bicep","text":"<pre><code>az group create --name Bicep --location eastus\n\naz deployment group create --resource-group Bicep --template-file main.bicep --parameters storageName=uniqueName\n</code></pre>"},{"location":"Manage-infrastructure-as-code-using-Azure-and-DSC/Implementing%20Bicep/#bicep-syntax","title":"Bicep syntax","text":"<pre><code>@minLength(3)\n@maxLength(11)\nparam storagePrefix string\n\nparam storageSKU string = 'Standard_LRS'\nparam location string = resourceGroup().location\n\nvar uniqueStorageName = '${storagePrefix}${uniqueString(resourceGroup().id)}'\n\nresource stg 'Microsoft.Storage/storageAccounts@2019-04-01' = {\n    name: uniqueStorageName\n    location: location\n    sku: {\n        name: storageSKU\n    }\n    kind: 'StorageV2'\n    properties: {\n        supportsHttpsTrafficOnly: true\n    }\n\n    resource service 'fileServices' = {\n        name: 'default'\n\n        resource share 'shares' = {\n            name: 'exampleshare'\n        }\n    }\n}\n\nmodule webModule './webApp.bicep' = {\n    name: 'webDeploy'\n    params: {\n        skuName: 'S1'\n        location: location\n    }\n}\n\noutput storageEndpoint object = stg.properties.primaryEndpoints\n</code></pre> <p>Legend:</p> <ul> <li>Scope: By default the target scope of all templates is set for <code>resourceGroup</code>, however, you can customize it by setting it explicitly. As other <code>allowed</code> values, <code>subscription</code>, <code>managementGroup</code>, and <code>tenant</code>.</li> <li>Parameters: They allow you to customize your template deployment at run time by providing potential values for names, location, prefixes, etc.</li> <li>Variables: Similar to parameters, variables play a role in making a more robust and readable template. Any complex expression can be stored in a variable and used throughout the template. When you define a variable, the type is inferred from the value.</li> <li>Resources: The resource keyword is used when you need to declare a resource in your templates. The resource declaration has a symbolic name for the resource that can be used to reference that resource later either for defining a subresource or for using its properties for an implicit dependency like a parent-child relationship.</li> <li>Modules: If you want truly reusable templates, you can't avoid using a module. Modules enable you to reuse a Bicep file in other Bicep files. In a module, you define what you need to deploy, and any parameters needed and when you reuse it in another file, all you need to do is reference the file and provide the parameters.</li> <li>Outputs: You can use outputs to pass values from your deployment to the outside world, whether it is within a CI/CD pipeline or in a local terminal or Cloud Shell. That would enable you to access a value such as storage endpoint or application URL after the deployment is finished.</li> </ul>"},{"location":"Manage-infrastructure-as-code-using-Azure-and-DSC/Implementing%20Desired%20State%20Configuration%20%28DSC%29/","title":"Implementing Desired State Configuration (DSC)","text":""},{"location":"Manage-infrastructure-as-code-using-Azure-and-DSC/Implementing%20Desired%20State%20Configuration%20%28DSC%29/#configuration-drift","title":"Configuration drift","text":"<p>Configuration drift is the process of a set of resources changing over time from their original deployment state. It can be because of changes made manually by people or automatically by processes or programs.</p> <p>Eventually, an environment can become a snowflake. A snowflake is a unique configuration that cannot be reproduced automatically and is typically a result of configuration drift.</p>"},{"location":"Manage-infrastructure-as-code-using-Azure-and-DSC/Implementing%20Desired%20State%20Configuration%20%28DSC%29/#solutions-for-managing-configuration-drift","title":"Solutions for managing configuration drift","text":"<ul> <li>Windows PowerShell Desired State Configuration. It's a management platform in PowerShell that enables you to manage and enforce resource configurations. </li> <li>Azure Policy. Use Azure Policy to enforce policies and compliance standards for Azure resources. </li> </ul>"},{"location":"Manage-infrastructure-as-code-using-Azure-and-DSC/Implementing%20Desired%20State%20Configuration%20%28DSC%29/#desired-state-configuration-dsc","title":"Desired State Configuration (DSC)","text":"<p>Desired State Configuration (DSC) is a configuration management approach that you can use for configuration, deployment, and management of systems to ensure that an environment is maintained in a state that you specify (defined state) and doesn't deviate from that state.</p> <p>Windows PowerShell DSC is a management platform in PowerShell that provides desired State. PowerShell DSC lets you manage, deploy, and enforce configurations for physical or virtual machines, including Windows and Linux.</p>"},{"location":"Manage-infrastructure-as-code-using-Azure-and-DSC/Implementing%20Desired%20State%20Configuration%20%28DSC%29/#dsc-components","title":"DSC components","text":"<ul> <li>Configurations. These are declarative PowerShell scripts that define and configure instances of resources. Upon running the configuration, DSC (and the resources being called by the configuration) will apply the configuration, ensuring that the system exists in the state laid out by the configuration. DSC configurations are also idempotent: The Local Configuration Manager (LCM) will ensure that machines are configured in whatever state the configuration declares.</li> <li>Resources. They contain the code that puts and keeps the target of a configuration in the specified state. Resources are in PowerShell modules and can be written to a model as generic as a file or a Windows process or as specific as a Microsoft Internet Information Services (IIS) server or a VM running in Azure.</li> <li>Local Configuration Manager (LCM). The LCM runs on the nodes or machines you wish to configure. It's the engine by which DSC facilitates the interaction between resources and configurations. The LCM regularly polls the system using the control flow implemented by resources to maintain the state defined by a configuration. If the system is out of state, the LCM calls the code in resources to apply the configuration according to specified.</li> </ul>"},{"location":"Manage-infrastructure-as-code-using-Azure-and-DSC/Implementing%20Desired%20State%20Configuration%20%28DSC%29/#two-ways-of-implementing-dsc","title":"Two ways of implementing DSC","text":"<ul> <li>Push mode: A user actively applies a configuration to a target node and pushes out the configuration.</li> <li>Pull mode is where pull clients are automatically configured to get their desired state configurations from a remote pull service. This remote pull service is provided by a pull server that acts as central control and manager for the configurations, ensures that nodes conform to the desired state, and reports on their compliance status. The pull server can be set up as an SMB-based pull server or an HTTPS-based server. HTTPS-based pull-server uses the Open Data Protocol (OData) with the OData Web service to communicate using REST APIs. It's the model we're most interested in, as it can be centrally managed and controlled. The following diagram provides an outline of the workflow of DSC pull mode.</li> </ul>"},{"location":"Manage-infrastructure-as-code-using-Azure-and-DSC/Implementing%20Desired%20State%20Configuration%20%28DSC%29/#azure-automation-state-configuration","title":"Azure Automation State configuration","text":"<p>Azure Automation State configuration DSC is an Azure cloud-based implementation of PowerShell DSC, available as part of Azure Automation. Azure Automation State configuration allows you to write, manage, and compile PowerShell DSC configurations, import DSC Resources, and assign configurations to target nodes, all in the cloud.</p> <p>The general process for how Azure Automation State configuration works is as follows:</p> <ol> <li>Create a PowerShell script with the configuration element.</li> <li>Upload the script to Azure Automation and compile the script into a MOF file. The file is transferred to the DSC pull server, provided as part of the Azure Automation service.</li> <li>Define the nodes that will use the configuration, and then apply the configuration.</li> </ol>"},{"location":"Manage-infrastructure-as-code-using-Azure-and-DSC/Implementing%20Desired%20State%20Configuration%20%28DSC%29/#dsc-configuration-file","title":"DSC configuration file","text":"<p>Let's start with the following example configuration:</p> <pre><code>configuration LabConfig\n    {\n        Node WebServer\n        {\n            WindowsFeature IIS\n            {\n                Ensure = 'Present'\n                Name = 'Web-Server'\n                IncludeAllSubFeature = $true\n            }\n        }\n    }\n</code></pre> <p>Legend:</p> <ul> <li>Configuration block. The Configuration block is the outermost script block. In this case, the name of the configuration is LabConfig. Notice the curly brackets to define the block.</li> <li>Node block. There can be one or more Node blocks. It defines the nodes (computers and VMs) that you're configuring. In this example, the node targets a computer called WebServer. You could also call it localhost and use it locally on any server.</li> <li>Resource blocks. There can be one or more resource blocks. It's where the configuration sets the properties for the resources. In this case, there's one resource block called WindowsFeature. Notice the parameters that are defined. </li> </ul>"},{"location":"Manage-infrastructure-as-code-using-Azure-and-DSC/Implementing%20Desired%20State%20Configuration%20%28DSC%29/#hybrid-management","title":"Hybrid management","text":"<p>The Hybrid Runbook Worker feature of Azure Automation allows you to run runbooks that manage local resources in your private data center on machines located in your data center. Azure Automation stores and manages the runbooks and then delivers them to one or more on-premises machines.</p> <p></p>"},{"location":"Manage-infrastructure-as-code-using-Azure-and-DSC/Implementing%20Desired%20State%20Configuration%20%28DSC%29/#hybrid-runbook-cworker-workflow-and-characteristics","title":"Hybrid Runbook cWorker workflow and characteristics","text":"<ul> <li>You can select one or more computers in your data center to act as a Hybrid Runbook Worker and then run runbooks from Azure Automation.</li> <li>Each Hybrid Runbook Worker is a member of a Hybrid Runbook Worker group, which you specify when you install the agent.</li> <li>A group can include a single agent, but you can install multiple agents in a group for high availability.</li> <li>There are no inbound firewall requirements to support Hybrid Runbook Workers, only Transmission Control Protocol (TCP) 443 is required for outbound internet access.</li> <li>The agent on the local computer starts all communication with Azure Automation in the cloud.</li> <li>When a runbook is started, Azure Automation creates an instruction that the agent retrieves. The agent then pulls down the runbook and any parameters before running it.</li> </ul>"}]}